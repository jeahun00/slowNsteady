---
tags:
  - Computer_Vision
  - Deep_Learning
  - Retrieval
  - Contrastive_Learning
---
<mark style='background:var(--mk-color-orange)'>ê¸°ì¡´ ë°©ë²•ë¡ </mark>
<mark style='background:var(--mk-color-green)'>ì´ ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ë°©ë²•</mark>
<span style='color:var(--mk-color-teal)'>ì œì‹œí•œ ë°©ë²•ì˜ ì¥ì </span>
<span style='color:var(--mk-color-red)'>ì œì‹œí•œ ë°©ë²•ì˜ ë‹¨ì </span>

TASK: Cross-modal retrieve


# Abstract
* Cross-modal retrieval method ëŠ” ì—¬ëŸ¬ modality ì— ëŒ€í•´ ì¼ë°˜ì ì¸ representation ì„ ë§Œë“¤ì–´ ë‚¸ë‹¤.
	* íŠ¹íˆ vision, language modal ì—ì„œ
* <span style='color:var(--mk-color-red)'>image ì™€ ê·¸ caption ì— ëŒ€í•´  ëŒ€ì‘ê´€ê³„ê°€ ë³µì¡í•œ ì </span>ì€ <span style='color:var(--mk-color-red)'>Cross-modal retrieve ë¥¼ ë” challenging</span> í•˜ê²Œ ë§Œë“ ë‹¤.
	* image-caption ì€ 1-1 ì´ ì•„ë‹Œ 1-N ê´€ê³„ì´ê¸° ë•Œë¬¸

* ìœ„ì™€ ê°™ì€ ì´ìœ ë¡œ ì´ ë…¼ë¬¸ì—ì„œëŠ” <span style='color:var(--mk-color-red)'>deterministic function ì€ one-to-many ë¥¼ íƒì§€í•  ìˆ˜ ì—†ê¸°ì—</span> ì¢‹ì§€ ì•Šì€ ëª¨ë¸ì´ë¼ê³  ë§í•œë‹¤.
* ë”°ë¼ì„œ ì´ ë…¼ë¬¸ì—ì„œëŠ” <mark style='background:var(--mk-color-green)'>Probabilistic Cross-Modal Embedding(PCME) ì„ ì œì•ˆ</mark>í•œë‹¤.
* ë˜í•œ <span style='color:var(--mk-color-red)'>ê¸°ì¡´ì˜ ë°ì´í„°ì…‹ì´ non-exhaustive annotation ì„ ê°€ì§€ê¸°ì—</span> <mark style='background:var(--mk-color-green)'>ìƒˆë¡œìš´ dataset ì¸ CUB dataset ì„ ì œì•ˆ</mark>í•œë‹¤.

* REF:
	* [deterministic model vs. probabilistic model](https://velog.io/@cardy20/%EC%A7%80%EC%8B%9D-%EC%A1%B0%EA%B0%81-Activation-function)
		* deterministic model: í™•ë¥ ì ì¸ ìš”ì¸ì´ ì—†ì–´ (ë™ì¼ input)-(ë™ì¼ output) ì„ ë³´ì¥í•˜ëŠ” ëª¨ë¸
		* probabilistic model: í™•ë¥ ì ì¸ ìš”ì¸ì´ ì²¨ê°€ë˜ì–´ (ë™ì¼ input)-(ë™ì¼ output) ì„ ë³´ì¥í•˜ì§€ ëª»í•˜ê³  random í•œ ìš”ì†Œê°€ ì²¨ê°€ë˜ëŠ” ëª¨ë¸
# 1. Introduction
* cross-modal retrieval ì€ ì–´ë ¤ìš´ taskì´ë‹¤.
	* cross-modal retrieval ì˜ ì˜ˆì‹œ
		* query: image, database: text -> image ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì¥ì„ ì°¾ëŠ” task
		* query: text, database: image -> text ë¥¼ ì£¼ì–´ì£¼ê³  ê·¸ì— í•´ë‹¹í•˜ëŠ” image ë¥¼ ì°¾ëŠ” task
* ìœ„ì˜ task ì—ì„œ ì–´ë ¤ì›€ì„ ì£¼ëŠ” ëŒ€ë¶€ë¶„ì˜ ì´ìœ ëŠ” multi modal ì— ë§ëŠ” representation ì„ ìƒì„±í•˜ëŠ” ê²ƒ.
* ì¦‰, multiple modality ë¥¼ ìœ„í•œ common representation ì„ ë§Œë“œëŠ” ê²ƒì€ chanllenging í•˜ë‹¤.
	* e.g. ì•„ë˜ ì´ë¯¸ì§€ì™€ ê·¸ì— ëŒ€í•œ caption ì„ ìƒê°í•´ë³´ì
	* ì´ë¯¸ì§€ëŠ” ê³ ì •ë˜ì–´ ìˆê¸°ì— 1
	* í•˜ì§€ë§Œ ì´ì— ëŒ€í•œ caption ì€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤.
	* ì¦‰ image-to-text ë‚˜ text-to-image ëŠ” ê·¸ ëŒ€ì‘ì´ one-to-many ê°€ ë  ìˆ˜ ë°–ì— ì—†ë‹¤.

![[Pasted image 20240426134439.png|500]]
* one-to-many representation ì„ ë§Œë“¤ê¸° ìœ„í•´ PVSE(Polysemous Visual-Semantic Embeddings) ê°€ ì œì•ˆë¨.
* ì´ ë°©ì‹ì€ given input ì— ëŒ€í•´ K ê°œì˜ proposal ì„ ì œì•ˆí•˜ëŠ” ë°©ë²• ì œì•ˆ
* ì´í›„ì— ì´ëŸ¬í•œ ë°©ì‹ì€ ë§ì€ ë°œì „ì„ ì´ë£¨ì—ˆë‹¤.
* í•˜ì§€ë§Œ computational cost ê°€ ë§¤ìš° ì»¸ë‹¤.

* ì´ì— ì´ ë…¼ë¬¸ì—ì„œëŠ” <mark style='background:var(--mk-color-green)'>Probabilistic Cross-Modal Embedding (PCME)</mark> ë¥¼ ì œì•ˆí•œë‹¤.
* ì´ ë°©ë²•ì€ ì•„ë˜ íŠ¹ì§•ì„ ê°€ì§„ë‹¤.
	* effective representation tools
	* explicit í•œ many-to-many representation ì´ í•„ìš” ì—†ë‹¤.
* ë˜í•œ ì´ ë°©ë²•ë¡ ì€ ì•„ë˜ì™€ ê°™ì€ ì¥ì ì„ ê°€ì§„ë‹¤.
	* ì²«ì§¸, PCMEëŠ” ì¿¼ë¦¬ì˜ ì‹¤íŒ¨ ê°€ëŠ¥ì„±ì´ë‚˜ ë‚œì´ë„ë¥¼ ì¶”ì •í•˜ëŠ” ë“± ìœ ìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆëŠ” ë¶ˆí™•ì‹¤ì„± ì¶”ì •ì¹˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤. 
	* ë‘˜ì§¸, probabilistic model ì€ ë” í’ë¶€í•œ ì„ë² ë”© ê³µê°„ìœ¼ë¡œ ì´ë£¨ì–´ì§„ë‹¤
		* deterministic representationì€ ìœ ì‚¬ì„± ê´€ê³„ë§Œì„ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
	* ì…‹ì§¸, PCMEëŠ” deterministic retrieval systems ì„ ë³´ì™„í•©ë‹ˆë‹¤.

* ë˜í•œ ì´ ë…¼ë¬¸ì—ì„œëŠ” <span style='color:var(--mk-color-red)'>non-exhaustive annotation ì„ ê°€ì§€ëŠ” MS-COCO</span> ëŒ€ì‹  <mark style='background:var(--mk-color-green)'>ìƒˆë¡œìš´ cross modality retreival benchmark(Using CUB) ë¥¼ ì œì•ˆ</mark>í•œë‹¤.


# 2. Related Work

# 3. Method

$$
\mathcal{D=(C,I)}
$$
* $\mathcal{D}$ : vision and language dataset
* $\mathcal{C}$ : set of captions
* $\mathcal{I}$ : set of images

$$
\begin{align*}
c\in\mathcal{C},i\in\mathcal{I} \\
\tau(c) \subseteq \mathcal{I}, \tau(i) \subseteq \mathcal{C}\\
\end{align*}
$$

* $\tau(c)$ : caption $c$ ì™€ ê´€ë ¨ëœ image(image ì´ë¯€ë¡œ image set $\mathcal{I}$ ì— ì†í•¨)
* $\tau(i)$ : image $i$ ì™€ ê´€ë ¨ëœ caption(caption ì´ë¯€ë¡œ caption set $\mathcal{C}$ ì— ì†í•¨)

* ëª¨ë“  query $q$ ì— ëŒ€í•´ cross-modal match ëŠ” 2ê°œì´ìƒ ì¡´ì¬í•œë‹¤.
* ë˜í•œ $q$ ëŠ” image ë‚˜ caption ì´ ë  ìˆ˜ ìˆë‹¤.
$$
|\tau(q)>1|
$$
* cross-modal retrieval method ëŠ” embedding space $\mathbb{R}^D$ ë¥¼ í•™ìŠµí•˜ì—¬ 
* ë‘ vector $f_\mathcal{V}, f_\mathcal{T}$ ì˜ similarity ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ ì£¼ìš” ëª©í‘œì´ë‹¤. 

## 3.1. Building blocks for PCME
* ì´ section ì—ì„œëŠ” PCME ì˜ ì£¼ìš” êµ¬ì„±ìš”ì†Œì¸ ì•„ë˜ 2ê°€ì§€ì— ëŒ€í•œ ì„¤ëª…ì œì‹œ
	1. joint visual-textual embeddings 
	2. probabilistic embeddings

### 3.1.1. Joint visual-textual embeddings
![[Pasted image 20240426150657.png]]
* **visual encoder** $f_\mathcal{V}$
	1. image $i$ ë¥¼ ResNet image encoder ë¡œ encoding
		* $i$ : input image
		* $g_\mathcal{V}$ : ResNet image encoder
		* $z_v=g_\mathcal{V}(i)$ : image encoder $g_\mathcal{V}$ ë¥¼ ê±°ì¹˜ê³  ë‚˜ì˜¨ image $i$ ì˜ feature map
	2. distribution predictor $h_\mathcal{V}$ ë¥¼ í†µí•´ distribution ì„ ì˜ˆì¸¡
		* $h_\mathcal{V}$ : GAP(Global Average Pooling) ì´í›„ì˜ linear layer
		* ì°¸ê³  : ìœ„ì˜ ê·¸ë¦¼ì€ PCME ë¥¼ ì ìš©í•˜ì—¬ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ê¹Œì§€ êµ¬í•œ ê·¸ë¦¼
	
![[Pasted image 20240426151547.png]]
* **Textual encoder** $f_\mathcal{T}$
	1. caption $c$ ë¥¼ pre-trained ëœ GloVe ë¡œ encoding
		* $c$ : input caption
		* $g_\mathcal{T}$ : pre-trained GloVe
		* $z_t=g_\mathcal{T}(c)$ : caption encoder $g_\mathcal{T}$ ë¥¼ ê±°ì¹˜ê³  ë‚˜ì˜¨ caption $c$ ì˜ word-level descriptor
		* $z_{t}\in\mathbb{R}^{L(c)\times d_t}$ 
			* $L(c)$ : number of words in $c$
	2. bidirectional GRU $h_\mathcal{T}$ ë¡œ ë¶€í„° sentence-level feature ì¶”ì¶œ
		* $h_\mathcal{T}$ : bidirectional GRU
		* ì°¸ê³  : ìœ„ì˜ ê·¸ë¦¼ì€ PCME ë¥¼ ì ìš©í•˜ì—¬ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ê¹Œì§€ êµ¬í•œ ê·¸ë¦¼

* **Losses used in prior work**
	* joint embedding ì˜ loss ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ contrastive ë‚˜ triplet loss ì‚¬ìš©
* **Polysemous visual-semantic embeddings (PVSE)**
	* PVSE: one-to-many matches for cross modal retrieval ì„ modeling í•¨
	1. visual case ì™€ textual case 2ê°œë¡œ ë‚˜ë‰œë‹¤.
	2. ê° case ì— ëŒ€í•´ $K$ ê°œì˜ proposal ì„ ì¶”ì¶œí•œë‹¤.
	3. textual, visual ì—ì„œ ì¶”ì¶œëœ embedding ì„ ì´ìš©í•˜ì—¬ MIL(Multiple Instance Learning) ì§„í–‰
		* $K^2$ ê°œì˜ possible visual-textual embedding pair ì¤‘ ê°€ì¥ ì¢‹ì€ pair ì„ íƒ
		* ê° pair ëŠ” supervised ë˜ì–´ ìˆë‹¤.
		* embedding ì¶”ì¶œ ê´€ë ¨ ìˆ˜ì‹ì€ ì•„ë˜ PVSE: visual case, PVSE: textual case ì°¸ê³ 

> ì°¸ê³ :  Multiple Instace Learning ì´ë€
> MIL ì´ë€ Multiple Instance Learningì˜ ì•½ìë¡œì¨, Instance ê°ê°ì— Labelì´ ë¶™ì–´ ê·¸ê²ƒì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ **ì—¬ëŸ¬ê°œì˜ Instanceê°€ bagì„ ì´ë£¨ê³  ì´ bagì— Labelì´ ë¶™ì–´ ê·¸ê²ƒì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ**ì„ ë§í•œë‹¤.
> REF: [MIL ì´ë€?](https://jellyho.com/blog/117/)

1. PVSE: visual case
$$
v^k = \mathsf{LN}(h_\mathcal{V}(z_v) + s(w^1 \mathsf{att}_\mathcal{v}^k(z_v) z_v))
$$
* $v^{k}\in\mathbb{R}^{D},k\in\{1,...,K\}$ : visual embedding
* $w^1 \in \mathbb{R}^{d_v \times D}$ : Fully connected layer ì˜ weight
* $s$ : sigmoid function 
* $\mathsf{LN}$ : LayerNorm
* $\mathsf{att}_\mathcal{V}^k$ : visual self-attention $\mathsf{att}_\mathcal{V}$ ì˜ k-th attention head 

2. PVSE: textual case
$$
t^k = \mathsf{LN}(h_\mathcal{T}(z_t) + s(w^1 \mathsf{att}_\mathcal{T}^k(z_t) z_t))
$$
* $t^{k}\in\mathbb{R}^{D},k\in\{1,...,K\}$ : visual embedding
* $w^1 \in \mathbb{R}^{d_v \times D}$ : Fully connected layer ì˜ weight
* $s$ : sigmoid function 
* $\mathsf{LN}$ : LayerNorm
* $\mathsf{att}_\mathcal{T}^k$ : textual self-attention $\mathsf{att}_\mathcal{V}$ ì˜ k-th attention head 


### 3.1.2. Probabilistic embeddings for a single modality
(ì•„ë˜ì—ì„œ ì„¤ëª…í•  ë‚´ìš©ë“¤ì€ ëŒ€ë¶€ë¶„ [Modeling Uncertainty With Hedged Instance Embedding](https://openreview.net/pdf?id=r1xQQhAqKX) ë…¼ë¬¸ì—ì„œ ë‚˜ì˜¤ëŠ” ë‚´ìš©ì´ë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ìœ„ì˜ ë…¼ë¬¸ì„ ì°¸ê³ í•˜ë¼)


---
![[Pasted image 20240426175510.png]]
### HIB ì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…
* 2 ì¢…ë¥˜ì˜ input ì— ëŒ€í•´ similarity ëŠ” ìœ ì§€í•˜ë˜ ë¶ˆí™•ì‹¤ì„±ì„ í¬í•¨í•œ representation ì„ ë§Œë“¤ì–´ ë‚´ëŠ” ëª¨ë¸
### pipeline of HIB 
1. CNN ì„ í†µí•´ K ê°œ ì˜ sampling ì¶”ì¶œ -> branch 1
2. ìœ„ì˜ ê³¼ì •ì„ ë™ì¼í•˜ê²Œ ë‹¤ë¥¸ branch ì—ì„œ ì§„í–‰ -> branch 2
	* ìœ„ ê³¼ì •ì€ contrastive learning ìœ¼ë¡œ í•™ìŠµ(ì•„ë˜ ì°¸ê³ )
	* $\hat{m}=1$ : matching, $\hat{m}=1$ : matching, $\hat{m}=0$ : non-matching
$$L_{\text{softcon}} = -\log p(m = \hat{m}|z_1, z_2) = \begin{cases} 
-\log p(m|z_1, z_2) & \text{if } \hat{m} = 1, \\
-\log (1 - p(m|z_1, z_2)) & \text{if } \hat{m} = 0, 
\end{cases}$$
3. ë§Œë“¤ì–´ì§„ branch 1 ì˜ Kê°œì˜ sample ê³¼ branch 2 ì˜ K ê°œì˜ branch ê°„ì˜ distance ê³„ì‚°
4. ê° point ê°„ì˜ similarity ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ probabilistic expression ìœ¼ë¡œ ì¹˜í™˜í•œë‹¤.
	* $\sigma$ ëŠ” sigmoid
$$p(m|z_1, z_2) := \sigma(-a||z_1 - z_2||_2 + b)$$
5. Match probability for probabilistic embedding(embedding ì— stochastic mapping ì¶”ê°€)
$$
p(m|x_1,x_2)=\int{p(m|z_1,z_2)p(z_1|x_1)p(z_2|x_2)dz_{1}dz_{2}}
$$
* ìœ„ì˜ ìˆ˜ì‹ì„ ì§ì ‘ ê³„ì‚°í•˜ëŠ” ê²ƒì€ ì–´ë ¤ì›€
* ë”°ë¼ì„œ ì•„ë˜ ë°©ì‹ìœ¼ë¡œ ê·¼ì‚¬ì¹˜ë¥¼ êµ¬í•¨
6. ì´í›„ monte carlo estimation ì„ ì§„í–‰í•˜ì—¬ ìœ„ì˜ ìˆ˜ì‹ì— ëŒ€í•œ ê·¼ì‚¬ì¹˜ë¥¼ êµ¬í•œë‹¤.
$$p(m|x_1, x_2) \approx \frac{1}{K^2} \sum_{k_1=1}^{K} \sum_{k_2=1}^{K} p(m|z_1^{(k_1)}, z_2^{(k_2)})$$
---
* ì´ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” PCME ëŠ” **ê° sample** ì„ **distribution ìœ¼ë¡œ modeling** í•œë‹¤.
* ì´ ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ modeling ì„ HIB(Hedged Instance Embedding)ì„ ì´ìš©í•œë‹¤.
* HIB ëŠ” probabilistic mapping $p_\theta(z|x)$ ë¥¼ train í•œë‹¤. ì´ëŠ” ì•„ë˜ íš¨ê³¼ë¥¼ ê°€ì§„ë‹¤.
	* pairwise semantic similarity ë³´ì¡´
	* ê³ ìœ í•œ ë¶ˆí™•ì‹¤ì„±ì„ ë³´ì¡´(random í•œ ì˜ì—­ì„ ë³´ì¡´í•œë‹¤.)

* **Soft contrastive loss**
$$
L_{\alpha\beta}(\theta) =
\begin{cases}
-\log p_\theta(m|x_\alpha, x_\beta) & \text{if } \alpha, \beta \text{ is a match} \\
-\log (1 - p_\theta(m|x_\alpha, x_\beta)) & \text{otherwise}
\end{cases}
$$
* $L_{\alpha\beta}(\theta)$ : soft-contrastive loss for pair of samples $(x_\alpha,x_\beta)$
* $(x_\alpha,x_\beta)$ : pair of samples

* **Factorizing match probability**
* match í™•ë¥ ì¸ $p_\theta(m|x_\alpha, x_\beta)$ ì•„ë˜ 2ê°€ì§€ë¡œ ë¶„ë¦¬í•¨
	* embedding $p_\theta(m|z_\alpha, z_\beta)$ 
	* encoder $p_\theta(z|x)$ ì— ê¸°ë°˜í•œ match probability
* ê·¸ í›„ Monte Carlo estimation ìœ¼ë¡œ ì¶”ì •í•œë‹¤.
$$
p_\theta(m|x_\alpha, x_\beta) \approx \frac{1}{J^2} \sum_{j} \sum_{j'} p(m|z_\alpha^j, z_\beta^{j'})
$$
* $p_\theta(m|x_\alpha, x_\beta)$ : match í™•ë¥ 
* $p_\theta(m|z_\alpha, z_\beta)$ : embedding
* $z^j$ : embedding distribution $p_\theta{(z|x)}$ ë¡œ ë¶€í„° sampling í•œ sample

* **Match probability from Euclidean distance**
* sample-wise match probability ë¥¼ ì•„ë˜ì™€ ê°™ì´ ì—°ì‚°í•œë‹¤.
$$
p(m|z_\alpha, z_\beta) = s(-a||z_\alpha - z_\beta||_2 + b)
$$
* $a, b$ : learnable scalar
* $s(\cdot)$ : sigmoid function

## 3.2. Probabilistic cross-modal embedding(PCME)
![[Pasted image 20240426205401.png]]
* ì•ì„  3.1. ì ˆì—ì„œ <mark style='background:var(--mk-color-green)'>embedding space ë¥¼ ë§Œë“œëŠ” ë°©ë²•</mark>ê³¼ <mark style='background:var(--mk-color-green)'>single modality ì—ì„œ probabilistic embedding ì„ ë§Œë“œëŠ” ë°©ë²•</mark>ì„ ê¸°ìˆ í•˜ì˜€ë‹¤.
* 3.2. ì ˆì—ì„œëŠ” multi modality embedding space(joint embedding space) ë¥¼ probabilistic representation with PCME ë¡œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•œë‹¤.

### 3.2.1. Model architecture
* PCME ëŠ” ì•„ë˜ 2ê°€ì§€ embedding ì„ ë™ì¼í•œ embedding space $\mathbb{R}^D$ ì—ì„œ normal distribution ìœ¼ë¡œ parameterize í•˜ì—¬ í‘œí˜„í•œë‹¤.
* ì´ ê³¼ì •ì—ì„œ í‰ê·  vector ì™€ diagonal covariance matrices ë¥¼ êµ¬í•œë‹¤.
	* $p(v|i)$ : image embedding space
	* $p(t|c)$ : textual embedding space
$$
\begin{align*}
p(v|i) \sim \mathcal{N} (h_\mathcal{V}^\mu (z_v), \text{diag}(h_\mathcal{V}^\sigma (z_v)))\\
p(t|c) \sim \mathcal{N} (h_\mathcal{T}^\mu (z_t), \text{diag}(h_\mathcal{T}^\sigma (z_t)))
\end{align*}
$$

* $z_v=g_\mathcal{V}(i)$ : feature map of image $i$
* $z_t=g_\mathcal{T}(c)$ : feature sequence of caption $c$
* $h^\mu,h^\sigma$ : mean, variance vector

* **Local attention branch**
![[Pasted image 20240426213514.png]]
![[Pasted image 20240426213913.png]]
* 3.1.1 ì ˆì˜ PVSE architecture ì—ì„œ ì˜ê°ì„ ë°›ì•„ ê·¸ êµ¬ì¡°ë¥¼ ì°¨ìš©í•œë‹¤.
* PVSE architecture ì—ì„œ image ì—ì„œëŠ” GAP & FC, caption ì—ì„œëŠ” Bi-GRU ë¡œ í–¥í•˜ëŠ” ë°©í–¥ì€ ì™„ì „íˆ ê°™ì€ êµ¬ì¡°ë¥¼ ì°¨ìš©í•œë‹¤.
* ë‹¨, local attention branch ì—ì„œëŠ” self-attention based aggregation ì„ ì—°ì‚°í•œë‹¤.
* (ìœ„ì˜ ì´ë¯¸ì§€ì—ì„œëŠ” $v^{\mu},t^{\mu}$ ë§Œì„ êµ¬í–ˆì§€ë§Œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì•„ë˜ì²˜ëŸ¼ $v^\sigma,t^\sigma$ ë¥¼ ëª¨ë‘ êµ¬í•œë‹¤.)
![[Pasted image 20240426214359.png|500]]

* **Module for $\mu$ versus $\sigma$** 
* $h^\mu_\mathcal{V},h^\mu_\mathcal{T}$ ë¥¼ êµ¬í•  ë•ŒëŠ” sigmoid, LayerNorm ê³¼ L2 projection ì„ ì‚¬ìš©í•œë‹¤.
* í•˜ì§€ë§Œ, $h^\sigma_\mathcal{V},h^\sigma_\mathcal{T}$ ë¥¼ êµ¬í•  ë•Œ <mark style='background:var(--mk-color-red)'>sigmoid, LayerNorm ê³¼ L2 projection ì„ ì‚¬ìš©</mark>í•  ë•Œ <span style='color:var(--mk-color-red)'>ê³¼ë„í•˜ê²Œ representation ì„ ì œí•œ</span>í•˜ëŠ” ê²½í–¥ì´ ìˆì—ˆë‹¤. 
* ë”°ë¼ì„œ $h^\sigma_\mathcal{V},h^\sigma_\mathcal{T}$ ë¥¼ êµ¬í•  ë•ŒëŠ” **sigmoid, LayerNorm ê³¼ L2 projection ì„ ëª¨ë‘ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤**.

* **Soft cross-modal contrastive loss**
$$L_{\alpha\beta}(\theta) =
\begin{cases}
-\log p_\theta(m|x_\alpha, x_\beta) & \text{if } \alpha, \beta \text{ is a match} \\
-\log (1 - p_\theta(m|x_\alpha, x_\beta)) & \text{otherwise}
\end{cases}$$
* joint probabilistic embedding ì€ image ì™€ caption ì˜ probabilistic embedding ì˜ mapping ì˜ parameter ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤.
	* Probabilistic embedding of image : $p(v|i) = p_\theta(v|i)$ 
	* Probabilistic embedding of caption : $p(t|c) = p_\theta(t|c)$
* loss function ì€ 3.1.2. ì ˆì—ì„œ ì†Œê°œí•œ loss function ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•œë‹¤.(ìœ„ì˜ ìˆ˜ì‹)
* match probabilities ëŠ” ì´ì œ ì•„ë˜ë¥¼ ë”°ë¥¸ë‹¤.
	* cross-modal pair $(i,c):\mathcal{L}_{emb}(\theta_v,\theta_t;i,c)$
	* $\theta=(\theta_v,\theta_t)$
$$p_\theta(m|i, c) \approx \frac{1}{J^2} \sum_j^J \sum_{j'}^J s(-a||v^j - t^{j'}||_2 + b)$$
* ìœ„ ìˆ˜ì‹ì˜ $v^j,t^{j'}$ ì€ ì•„ë˜ ìˆ˜ì‹ì„ ë”°ë¥¸ë‹¤.
$$
\begin{align*}
p(v|i) \sim \mathcal{N} (h_\mathcal{V}^\mu (z_v), \text{diag}(h_\mathcal{V}^\sigma (z_v)))\\
p(t|c) \sim \mathcal{N} (h_\mathcal{T}^\mu (z_t), \text{diag}(h_\mathcal{T}^\sigma (z_t)))
\end{align*}
$$
* ìµœì¢… loss ë¥¼ ì´ ë…¼ë¬¸ì— ë§ê²Œ ë°”ê¾¸ë©´ ì•„ë˜ì™€ ê°™ë‹¤.
$$
L_{ij}(\theta) =
\begin{cases}
-\log p_\theta(m|i,c) & \text{if } i, c \text{ is a match} \\
-\log (1 - p_\theta(m|i,c)) & \text{otherwise}
\end{cases}$$
# 4. Experiments
* ì´ section ì—ì„œëŠ” ì•„ë˜ í•­ëª©ì„ ë‹¤ë£¬ë‹¤.
	* experimental protocol
	* í˜„ì¬ ì‚¬ìš©ë˜ê³  ìˆëŠ” cross-modal retrieval benchmark ì™€ evaluation metric ì— ëŒ€í•œ ë¬¸ì œì 
	* ê·¸ í›„, CUB cross-modal retrieval task, COCO ì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼
	* embedding space ì— ëŒ€í•œ ì‹¤í—˜

## 4.1. Experimental protocol
* visual encoders: ResNet pre-trained on ImageNet
* text encoders: GloVe pre-trained with 2.2M vocabulary

1. COCO
* backbone: ResNet-152
* embedding dimension: $D=1024$
* optimizer: AdamP
2. CUB
* backbone: ResNet-50
* embedding dimension: $D=512$
* optimizer: AdamP

### 4.1.1. Metrics for cross-modal retrieval

---
![[Pasted image 20240427215558.png]]
#### ì°¸ê³ : Recall@k
* ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ê°’
$$
Recall@k=\frac{\#\ of\ recommended\ items\ @k\ that\ are\ relevant}{total\ \#\ of\ relevant\ items}
$$
* Recall ì€ ì‹¤ì œ ëª¨ë“  1 ì¤‘ì—ì„œ ë‚´ê°€ 1ë¡œ ì˜ˆì¸¡í•œ ê²ƒì´ ì–¼ë§ˆë‚˜ ë˜ëŠ”ì§€ ë¹„ìœ¨ì„ ë‚˜íƒ€ë‚¸ë‹¤.Â **Recall@KëŠ” ì‚¬ìš©ìê°€ ê´€ì‹¬ìˆëŠ” ëª¨ë“  ì•„ì´í…œ ì¤‘ì—ì„œ ë‚´ê°€ ì¶”ì²œí•œ ì•„ì´í…œ Kê°œê°€ ì–¼ë§ˆë‚˜ í¬í•¨ë˜ëŠ”ì§€**Â ë¹„ìœ¨ì„ ì˜ë¯¸í•œë‹¤. [ê·¸ë¦¼1]ì˜ ì˜ˆì‹œë¥¼ ë³´ë©´, ì‚¬ìš©ìê°€ ì¢‹ì•„í•œ ëª¨ë“  ì•„ì´í…œì´ 6ê°œ, ê·¸ ì¤‘ì—ì„œ ë‚˜ì˜ ì¶”ì²œì— í¬í•¨ë˜ëŠ” ì•„ì´í…œì´ 3ê°œë‹¤. ë”°ë¼ì„œ, Recall@5=0.5ê°€ ëœë‹¤.
#### ì°¸ê³ : Precision@k
$$
Precision@k=\frac{\#\ of\ recommended\ items\ @k\ that\ are\ relevant}{\#\ of\ recommended\ items\ @k}
$$
* Precisionì€ ë‚´ê°€ 1ë¡œ ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ì— ì‹¤ì œ 1ì´ ì–¼ë§ˆë‚˜ ìˆëŠ”ì§€ ë¹„ìœ¨ì„ ë‚˜íƒ€ë‚¸ë‹¤.Â **Precision@KëŠ” ë‚´ê°€ ì¶”ì²œí•œ ì•„ì´í…œ Kê°œ ì¤‘ì— ì‹¤ì œ ì‚¬ìš©ìê°€ ê´€ì‹¬ìˆëŠ” ì•„ì´í…œì˜ ë¹„ìœ¨**ì„ ì˜ë¯¸í•œë‹¤. [ê·¸ë¦¼1]ì˜ ì˜ˆì‹œì— ë”°ë¥´ë©´, ì¶”ì²œí•œ ì•„ì´í…œì€ 5ê°œ, ì¶”ì²œí•œ ì•„ì´í…œ ì¤‘ì—ì„œ ì‚¬ìš©ìê°€ ì¢‹ì•„í•œ ì•„ì´í…œì€ 3ê°œë‹¤. ë”°ë¼ì„œ, Precision@5=0.6ì´ ëœë‹¤.

#### ì°¸ê³ : R-precision
Â * R-precisionì´ë€, ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•˜ìë©´ **ëª¨ë“  pair ê°€ìš´ë° ground-truthì¸ pairê°€ Rê°œ ì¡´ì¬**í•œë‹¤ê³  í•  ë•Œ **cosine similarityë¥¼ ê¸°ì¤€ìœ¼ë¡œ pairë“¤ì„ ì •ë ¬í•œ** í›„ **ìƒìœ„ Rê°œì˜ pair ê°€ìš´ë° ì¡´ì¬í•˜ëŠ” ì•Œë§ì€ pair rê°œì— ëŒ€í•´ r/Rì„ ê³„ì‚°**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤

---
* Recall@k in retrieval task
* queryì— ëŒ€í•´ì„œ Kê°œì˜ nearest neighborì¤‘ ê°™ì€ categoryì˜ imageê°€ k ê°œ í¬í•¨ë˜ë©´ TP ë¡œ ì·¨ê¸‰. 
* ì´í›„ recall ê³„ì‚°
* $Recall=\frac{TP}{TP+FN}$: ëª¨ë¸ì´ ì •ë‹µì„ ë§ì¶˜ ê²ƒ ì¤‘ True ë¼ê³  ë¶„ë¥˜í•œ ìˆ˜ì¹˜
* $Precision=\frac{TP}{TP+FP}$

* Recall@k ëŠ” k ê°’ì´ í¬ë©´ í´ìˆ˜ë¡ COCO ì—ì„œ ì˜ëª»ëœ ì˜ˆì¸¡ì— ëŒ€í•´ ê´€ëŒ€í•´ì§„ë‹¤.
* ì´ëŠ” ì˜ëª» ê²€ìƒ‰ëœ sample ì— ëŒ€í•œ penalty ë¥¼ ë¶€ì—¬í•˜ì§€ ëª»í•œë‹¤.
* ì´ëŸ¬í•œ ì˜ëª»ëœ ê²€ìƒ‰ì— ëŒ€í•œ penalty ëŠ” precision@k ë¡œ ë³´ì™„í•œë‹¤.
* R-precision ì„ ë„ì…
	* ê° query q ì— ëŒ€í•´, ìƒìœ„ R ê°œì˜ retrieval í•­ëª©ì—ì„œ positive ì˜ ë¹„ìœ¨ì„ ì¸¡ì •
	* $R=|\tau{(q)}|$ : GT match ì˜ ìˆ˜
* ì´ëŸ¬í•œ R-precision ì´ ì˜ë¯¸ë¥¼ ê°€ì§€ê¸° ìœ„í•´ì„œëŠ” ëª¨ë“  positive pair ì— ëŒ€í•´ annotating ì´ ë˜ì–´ ìˆì–´ì•¼ í•œë‹¤.
* ë”°ë¼ì„œ ì´ ë…¼ë¬¸ì—ì„œëŠ” extra information(MS-COCO ì—ì„œëŠ” class label) ì„ ì´ìš©í•˜ì—¬ positive pair ì— ëŒ€í•œ ì¶”ê°€ì ì¸ annotating ì„ ì§„í–‰í•œë‹¤.
* pair $(i,c)$ ì— ëŒ€í•˜ì—¬ binary label vector ë¥¼ ìƒì„±í•œë‹¤. $y^{i},y^{c}\in\{0,1\}^{d_{label}}$
* ì´ë ‡ê²Œ ì£¼ì–´ì§„ binary label vector ëŠ” $\zeta$ ë¼ëŠ” margin ì„ ì£¼ì–´ ì—¬ëŸ¬ê°œì˜ ê¸°ì¤€ì„ ìƒì„±í•œë‹¤.
* $\zeta=\{0,1,2\}$ ì¼ ë•Œë¥¼ ëª¨ë‘ êµ¬í•œ í›„ í‰ê· ì„ ë‚´ëŠ” ê²ƒì„ ìµœì¢…ì ì¸ metric ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.
	* 0,1,2 ëŠ” **embedding ê°„ì˜ Hamming distance** ë¥¼ ì˜ë¯¸í•¨
	* ì˜ˆì‹œë¡œ ë¼ë²¨ì´ {ìƒˆ, ê³ ì–‘ì´, ê°•ì•„ì§€, ê¸°ë¦°, ì–¼ë£©ë§} í˜•íƒœì¸ 1D vector ë¼ í•´ ë³´ì
	* ì•„ë˜ ì‚¬ì§„ìœ¼ë¡œ $\zeta$ ì˜ ê²½ìš°ë¥¼ ì„¤ëª…í•˜ë©´
		* $GT-vector=\{1,0,0,1,1\}$
		* $y^i$: ì•„ë˜ì‚¬ì§„, $y^c$: ê¸°ë¦°ê³¼ ìƒˆì™€ ì–¼ë£©ë§ì´ ìˆë‹¤, binary label vector: $\{1,0,0,1,1\}$
			* ì´ëŠ” $\zeta=0$ ì¸ ê²½ìš°ì—ë§Œ positive pair ë¡œ ì·¨ê¸‰í•œë‹¤.
			* ì¦‰, ì™„ì „íˆ ë™ì¼í•´ì•¼ë§Œ positive ë¡œ ì·¨ê¸‰
		* $y^i$: ì•„ë˜ì‚¬ì§„, $y^c$: ê¸°ë¦°ê³¼ ìƒˆê°€ ìˆë‹¤, binary label vector: $\{1, 0, 0, 1, 0\}$
			* ì´ëŠ” $\zeta\geq1$ ì¸ ê²½ìš°ì—ë§Œ positive pair ë¡œ ì·¨ê¸‰í•œë‹¤.
			* ì¦‰, $\zeta=0$ ì¸ ê²½ìš°ì—ëŠ” $GT:\{1,0,0,1,1\},predict:\{1,0,0,1,0\}$ ê°„ì˜ hamming distance ê°€ 1ì´ë¯€ë¡œ negative pair ë¡œ ì·¨ê¸‰í•œë‹¤.
* ìœ„ì™€ ê°™ì€ $\zeta$ ë¥¼ ë‘” í‰ê°€ì§€í‘œë¥¼ 
![[Pasted image 20240427162252.png]]

* **ì´ ë…¼ë¬¸ì—ì„œì˜ ì˜ˆì‹œ**
* class label vector = {ì„ ìˆ˜, ë°°íŠ¸, ê¸€ëŸ¬ë¸Œ, ê³µ, ë°´ì¹˜}
* ìœ„ë¥¼ ê°„ëµí™”í•˜ì—¬ binary vector ë¡œ í‘œì‹œí•œë‹¤.
* ì•„ë˜ GT ë¥¼ ì´ vector ë¡œ í‘œì‹œí•˜ë©´ {1,1,1,1,0} ì´ë‹¤.({ì„ ìˆ˜O,ë°°íŠ¸O,ê¸€ëŸ¬ë¸ŒO,ê³µO,ë°´ì¹˜X})
* ì°¨ë¡€ë¡œ $\zeta=0$ ì¸ ê²½ìš°ëŠ” {1,1,1,1,0}, ì¦‰, GT ì™€ì˜ hamming distance ê°€ 0 ì¸ ê²½ìš°ì´ë‹¤.
	* hamming distance {1,1,1,1,0}<->{1,1,1,1,0} == 0
* $\zeta=1$ ì¸ ê²½ìš°ëŠ” {1,1,1,0,0}, ì¦‰, GT ì™€ì˜ hamming distance ê°€ 1 ì¸ ê²½ìš°ì´ë‹¤.
	* hamming distance {1,1,1,<span style='color:var(--mk-color-red)'>1</span>,0}<->{1,1,1,<span style='color:var(--mk-color-red)'>0</span>,0} == 0
* $\zeta=2$ ì¸ ê²½ìš°ëŠ” {1,1,1,0,1}, ì¦‰, GT ì™€ì˜ hamming distance ê°€ 2 ì¸ ê²½ìš°ì´ë‹¤.
	* hamming distance {1,1,1,<span style='color:var(--mk-color-red)'>1,0</span>}<->{1,1,1,<span style='color:var(--mk-color-red)'>0,1</span>} == 2
![[Pasted image 20240427222758.png]]
### 4.1.2. Cross-modal retrieval benchmarks

#### COCO Captions
* image-to-text: 1-5
	* image ê°€ 1ê°œ ì£¼ì–´ì¡Œì„ ë•Œ caption ì€ 5ê°œ
* text-to-image: 1-1
	* caption ì´ ì£¼ì–´ì¡Œì„ ë•Œ ê·¸ì— ìƒì‘í•˜ëŠ” ì´ë¯¸ì§€ëŠ” 1ê°œ
* ìœ„ì˜ ì¡°ê±´ì— ë§Œì¡±í•˜ì§€ ì•ŠëŠ” pair ë“¤ì€ ê·¸ ìœ ì‚¬ì„±ê³¼ ê´€ë ¨ì—†ì´ ëª¨ë‘ negative ë¡œ ì·¨ê¸‰
* ì˜ˆì‹œ
![[Pasted image 20240427163556.png|500]]
* ìœ„ì˜ ê·¸ë¦¼ì—ì„œ ëª¨ë“  image ì™€ ëª¨ë“  caption ì€ ì„œë¡œ ìœ ì‚¬ì„±ì´ ìˆì–´ positive ì²˜ëŸ¼ ë³´ì¸ë‹¤.
* í•˜ì§€ë§Œ COCO ì—ì„œ labeling ì€ ê°€ëŠ¥í•œ 16ê°œì˜ ê²½ìš°ì˜ ìˆ˜ ì¤‘ 12ê°œë¥¼ ë¬´ì‹œí•˜ê³  ë‹¨ 4ê°œì˜ ê²½ìš°ë§Œì„ positive ë¡œ ì·¨ê¸‰í•œë‹¤.
* ì´ëŸ¬í•œ ê²½ìš° train ì‹œ<span style='color:var(--mk-color-red)'> ì¡ìŒì´ ë§ê³ </span>, <span style='color:var(--mk-color-red)'>ì‹ ë¢°ì„±ì´ ë–¨ì–´ì§€ëŠ” ë¬¸ì œ</span> ë°œìƒí•œë‹¤.

* ì´ì— ì´ ë…¼ë¬¸ì—ì„œëŠ” CUB200-2011 ì„ ì œì•ˆí•œë‹¤.
#### CUB captions
* CUB ëŠ” í•œ image ì— 10ê°œì˜ caption ì„ ë¶€ì—¬í•œë‹¤.
* ë˜í•œ 11,788ê°œì˜ image ì— ëŒ€í•´ 200ê°œì˜ ì„¸ë°€í•œ class ë¥¼ ë¶€ì—¬í•œë‹¤.
* ì´ë¥¼ í†µí•´ caption ê³¼ image class ë‚´ì˜ ê· í˜•ì„ ë§ì¶”ì–´ false-positive ë¥¼ ì–µì œí•œë‹¤.

## 4.2. Result on CUB
![[Pasted image 20240427164247.png]]
* ìœ„ì˜ í‘œëŠ” match probabilistic ì— ëŒ€í•œ ì •ë‹¹ì„±ì„ ë³´ì—¬ì£¼ëŠ” í‘œì´ë‹¤.
* 2ê°œì˜ cross embedding(text-imgae) ê°„ì˜ ìœ ì‚¬ì„±ì„ íŒë‹¨í•  ë•Œ distance function ìœ¼ë¡œ ì–´ë– í•œ ë°©ë²•ë¡ ì„ ì‚¬ìš©í–ˆëŠ”ì§€ì— ëŒ€í•œ ê²°ê³¼ì´ë‹¤.
* ì´ ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ Match Prob ì´ ì„œëŠ¥ì´ ì œì¼ ì¢‹ìŒì„ ì•Œìˆ˜ ìˆë‹¤.(ë‹¨, ê³µê°„ë³µì¡ë„ëŠ” ì¼ë¶€ ì†í•´ë¥¼ ë³¸ë‹¤.)
* ê° column ì— ëŒ€í•œ ì„¤ëª…ì€ ì•„ë˜ ì°¸ê³ 
	* sampling: Monte-Carlo sampling ì‚¬ìš© ì—¬ë¶€
	* i2t: image to text, t2i: text to image

## 4.3. Result on COCO
![[Pasted image 20240427164614.png]]

* ìœ„ì˜ ê²°ê³¼ë¥¼ ë³´ë©´ PMRP ìˆ˜ì¹˜ëŠ” SOTA ì´ë‹¤. í•˜ì§€ë§Œ R@1 ì€ ìˆ˜ì¹˜ê°€ ë¹„êµì  ë‚®ë‹¤.
* ì´ëŸ¬í•œ ì´ìœ ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.
	* PMRP: ì™„ì „íˆ ì •í™•í•˜ì§„ ì•Šë”ë¼ë„ ë¬¸ë§¥ìƒìœ¼ë¡œ í­ ë„“ê²Œ ì¼ì¹˜í•˜ëŠ”ì§€ íŒë‹¨ì´ ê°€ëŠ¥
	* R@1: ì •ëŸ‰ì ìœ¼ë¡œ ìì—°ì–´ë¥¼ ì •í™•íˆ íŒë‹¨í•¨
	* ì¦‰, ì‹¤ì œë¡œ ì´ë¯¸ì§€ì™€ ìì—°ì–´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ matching í•˜ëŠ” ê²ƒì€ í•´ë‹¹ ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ëª¨ë¸ì´ë‹¤.
* ì „ì²´ì— ëŒ€í•œ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

![[Pasted image 20240427165322.png]]

## 4.4. Understanding the learned uncertainty








---


* limitation:
	* section 4.3. Result on COCO ì—ì„œ R@k ì— ëŒ€í•œ ì„±ëŠ¥ì´ SOTA ê°€ ì•„ë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œ ìƒˆë¡­ê²Œ ì œì‹œí•œ í‰ê°€ì§€í‘œë§Œ SOTA ë¥¼ ë‹¬ì„±í•˜ì˜€ë‹¤. ë§Œì•½ ì •ë§ë¡œ ìì—°ì–´ì™€ ì´ë¯¸ì§€ê°„ì˜ matching ì„ PMRP ê°€ ë” ì˜ í‰ê°€í•œë‹¤ë©´ PMRP ê°€ ë†’ê±°ë‚˜ ë‚®ì„ë•Œ, R@k ê°€ ë†’ê±°ë‚˜ ë‚®ì„ë•Œì˜ visualized image ê°€ ì œì‹œí•˜ì—¬ í™•ì‹¤í•˜ê²Œ ë³´ì—¬ì¤¬ì–´ì•¼ í•  ê²ƒ ê°™ë‹¤.


For each query ğ‘ measure the proportion of positives in the top ğ‘… retrieval items.
