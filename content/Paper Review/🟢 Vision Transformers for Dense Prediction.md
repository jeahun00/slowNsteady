
# Abstract
* dense prediction task ì—ì„œ CNN ëŒ€ì‹  transformer ë¥¼ ì‚¬ìš©í•˜ëŠ” dense vision transformer ë¥¼ ì†Œê°œí•œë‹¤.
* ì´ëŠ” ì—¬ëŸ¬ ë‹¨ê³„ì—ì„œ token ì„ ìˆ˜ì§‘í•˜ì—¬ image ì™€ ê°™ì€ representation ì„ êµ¬ì„±í•˜ê³ , ì´ë¥¼ conv decoder ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ í•´ìƒë„ì˜ ì˜ˆì¸¡ìœ¼ë¡œ ì ì§„ì ìœ¼ë¡œ ê²°í•©í•œë‹¤.

* dense vision transformer ëŠ” cnn ì— ë¹„í•´ global í•˜ê²Œ ì¼ê´€ëœ ì •ë³´ë¥¼ ì œê³µ
* ë˜í•œ ëŒ€ëŸ‰ì˜ í•™ìŠµë°ì´í„°ì…‹ì´ ì¡´ì¬í•  ë•Œ ìƒë‹¹í•œ ê°œì„ ì„ ë³´ì—¬ì¤€ë‹¤.

# 1. Introduction
* Dense prediction ì„ ìœ„í•œ architecture
	* encoder: large scale dataset ìœ¼ë¡œ pretrained ëœ image classification network(ì¦‰, backbone) ì‚¬ìš©
	* decoder: encoder ì˜ íŠ¹ì§•ì„ ì§‘ê³„í•˜ê³  final dense prediction ìœ¼ë¡œ ë³€í™˜

* ==dense prediction ì— ëŒ€í•œ êµ¬ì¡°ì  ì—°êµ¬ëŠ” ì£¼ë¡œ decoder ì™€ ê·¸ aggregation strategy ì— ì´ˆì ==ì„ ë§ì¶˜ë‹¤.
* í•˜ì§€ë§Œ backbone êµ¬ì¡° ìì²´ê°€ ëª¨ë¸ ì „ì²´ì— í° ì˜í–¥ì„ ë¯¸ì¹œë‹¤.
* ì´ëŠ” ì˜ëª»ëœ backbone ì˜ ì‚¬ìš©ìœ¼ë¡œ encoder ì—ì„œ ì†Œì‹¤ëœ ì •ë³´ëŠ” decoder ì—ì„œ íšŒë³µí•  ìˆ˜ ì—†ëŠ” ë¬¸ì œë¥¼ ì•¼ê¸°í•œë‹¤.

* ==CNN backbone ì€ input image ë¥¼ convolution ì„ í†µí•´ ì ì§„ì ìœ¼ë¡œ downsampling í•˜ëŠ” ë°©ë²•== ì‚¬ìš©
	* <mark style='background:var(--mk-color-teal)'>ì¥ì </mark>: receptive fieldì˜ ì ì§„ì  ì¦ê°€, high-level feature extract ë“±
	* <mark style='background:var(--mk-color-red)'>ë‹¨ì </mark>: 
		* deep cnn ì˜ ê²½ìš° resolution ê³¼ detail ì´ ì†ìƒ
		* classification task ì˜ ê²½ìš° í° ë¬¸ì œê°€ ë˜ì§€ ì•Šì§€ë§Œ <span style='color:var(--mk-color-red)'>pixel-level prediction ì„ ì§„í–‰í•´ì•¼ í•˜ëŠ” dense prediction task ì—ì„œëŠ” ì¹˜ëª…ì </span>
* ìœ„ì™€ ê°™ì€ ==CNN ì˜ ê³ ì§ˆì ì¸ ë‹¨ì ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì•„ë˜ ë°©ë²•ë¡  ì œì‹œ==
	* computation ìì›ì´ í—ˆìš©í•˜ëŠ” ë§Œí¼ì˜ ë†’ì€ í•´ìƒë„ì—ì„œ í›ˆë ¨
	* dilated convolution
	* skip connection
	* deep high-resolution representation(HR Net)
		* ì´ ë§í¬ ì°¸ê³ : [[ğŸŸ¢ Deep High-Resolution Representation Learning for Visual Recognition]]
* í•˜ì§€ë§Œ <span style='color:var(--mk-color-red)'>convolution ì—°ì‚°ì˜ íŠ¹ì„±ìƒ, í•œë²ˆì— ì¢ì€ ì˜ì—­ì˜ íŠ¹ì„±ë§Œì„ ë³¼ ìˆ˜ ìˆê¸°ì— ì—¬ì „íˆ dense prediction ì—ì„œ cnn ì˜ ì ìš©ì€ í•œê³„</span>ê°€ ìˆë‹¤.

* ì´ì— ì´ ë…¼ë¬¸ì—ì„œëŠ” <mark style='background:var(--mk-color-green)'>Dense Prediction Transformer(DPT)</mark> ë¥¼ ì†Œê°œí•œë‹¤.
	* ViT ë¥¼ encoder ì˜ backbone ìœ¼ë¡œ í™œìš©
	* ViT íŠ¹ì„±ìƒ ì´ˆê¸° image embedding ì¶”ì¶œ ì´í›„ì—ì„œëŠ” ëª…ì‹œì ì¸ downsampling ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ
	* ë˜í•œ Transformer ì˜ íŠ¹ì„±ìœ¼ë¡œ ì¸í•´ global information ì„ ì°¸ì¡°í•  ìˆ˜ ìˆë‹¤.
* ì´ëŸ¬í•œ backbone ì˜ í™œìš©ìœ¼ë¡œ monocular depth estimation, semantic segmentation ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ì´ëŒì–´ëƒ„


# 2. Related Work
* CNN ê¸°ë°˜ì˜ dense prediction
	* conv ë¡œ downsample ì´í›„ upsampling
* NLP ëª¨ë¸ì„ ìœ„í•œ transformer êµ¬ì¡°
	* vision ì„ ìœ„í•œ ViT

# 3. Architecture
* ê¸°ì¡´ì˜ dense prediction ì—ì„œ ì‚¬ìš©í•˜ë˜ encoder-decoder êµ¬ì¡°ëŠ” ìœ ì§€
* ë‹¨, encoder ì˜ backbone ìœ¼ë¡œ ViT ì‚¬ìš©
![[Pasted image 20240509193858.png]]

## Transformer encoder.
![[Pasted image 20240509202213.png|500]]
* transformer ì—ì„œëŠ” bag-of-words(ë‹¨ì–´ë“¤ì˜ ìˆœì„œëŠ” ê³ ë ¤í•˜ì§€ ì•Šê³  ë¹ˆë„ì— ì§‘ì¤‘í•œ ìˆ˜ì¹˜í™”ëœ í‘œí˜„) ì‚¬ìš©
* ViT ì—ì„œëŠ” ë‹¨ì–´ë“¤ ëŒ€ì‹ , **image ì—ì„œ crop ëœ patch** ë“¤ì´ **ë‹¨ì–´ì˜ ì—­í• ì„ ëŒ€ì‹ **í•œë‹¤.
* CNN ì—ì„œëŠ” ë„¤íŠ¸ì›Œí¬ë¥¼ í†µê³¼í•˜ë©´ í• ìˆ˜ë¡ ì ì§„ì ìœ¼ë¡œ receptive field ê°€ ì»¤ì§(ë„¤íŠ¸ì›Œí¬ë¥¼ í†µê³¼í• ìˆ˜ë¡ global feature ë¥¼ ì°¸ê³ í•  ìˆ˜ ìˆê²Œ ë¨)
* í•˜ì§€ë§Œ ViT ëŠ” patch ì˜ í•´ìƒë„ë¥¼ transformer ê³¼ì •ì¤‘ ì§€ì†í•´ì„œ ìœ ì§€í•˜ê¸°ì— ëª¨ë“  ê³¼ì • ì¤‘ global feature ë¥¼ ì°¸ê³ í•  ìˆ˜ ìˆë‹¤.

* ViT ì˜ êµ¬ì¡°
	* $p^2$ ìœ¼ë¡œ ì´ë¯¸ì§€ crop
	* ResNet-50 ìœ¼ë¡œ image embedding ì¶”ì¶œ
	* image patch ëŠ” íŠ¹ì •í•œ ìˆœì„œë¥¼ ê°€ì§€ì§€ ëª»í•¨-> ë”°ë¼ì„œ positional embedding ì„ ì¶”ê°€
	* ë˜í•œ, classification ì„ ìœ„í•´ **readout token** ì„ ì¶”ê°€í•œë‹¤.

* $l$ ë²ˆì§¸ transformer block ì˜ output: $t^l=\{t^l_0,...,t^l_{N_p}\}$
* classification ì„ ìœ„í•œ readout token: $t_0$
* $N_p = \frac{HW}{p^2}$

* ì´ ë…¼ë¬¸ì—ì„œëŠ” 3ê°€ì§€ì˜ ViT ë¥¼ ì‚¬ìš©
	* ì•„ë˜ 3ê°€ì§€ ì‘ì—…ì— ëŒ€í•´ $p=16$ ìœ¼ë¡œ ê³ ì •
	1. ViT-Base: 
		* Patch-based embedding(linear embedding ì¸ê°€? -> í™•ì¸ í•„ìš”)
		* 12ê°œì˜ transformer layer
		* feature size $D=768$
	2. ViT-Large: 
		* Patch-based embeddings
		* 24ê°œì˜ transformer layer
		* feature size $D=1024$
	3. ViT-Hybrid
		* ResNet-50 based embeddings
		* 12ê°œì˜ transformer layer
		* feature size : input image ì˜ $\frac{1}{16}$ ì— í•´ë‹¹í•˜ëŠ” feature
* ìœ„ì˜ 3ê°€ì§€ ì‘ì—… ëª¨ë‘ ì…ë ¥ patch ì˜ pixel ë³´ë‹¤ ë” í° feature size ë¥¼ ê°€ì§€ê¸°ì— ì•„ë˜ ì¥ì ì„ ê°€ì§
	* ê¸°ì¡´ì˜ ì •ë³´ ì¤‘ ìœ ìµí•œ ì •ë³´ë¥¼ ë³´ì¡´
	* ë”ìš± ë§ì€ ì •ë³´ë¥¼ ë°˜ì˜í•  ìˆ˜ ìˆë‹¤.

## Convolution decoder

* convolution decoder ì—ì„œëŠ” token ì§‘í•©ì„ ì—¬ëŸ¬ í•´ìƒë„ì—ì„œ image-likde feature representation ìœ¼ë¡œ assemble í•œë‹¤.
* ì´ëŸ¬í•œ feature representation ì€ ì ì§„ì ìœ¼ë¡œ final dense prediction ìœ¼ë¡œ í•©ì³ì§„ë‹¤.
* transformer ì˜ ì„ì˜ì˜ ê³„ì¸µì˜ ì¶œë ¥ token ì—ì„œ image-like representation ì„ ë³µì›í•˜ê¸° ìœ„í•´ 3ë‹¨ê³„ì˜ Reassemble operation ì„ ì œì•ˆ.


$$
\text{Reassemble}_s(t) = (\text{Resample} \circ \text{Concatenate} \circ \text{Read})(t)
$$
* $s$: input image ì— ëŒ€í•œ íšŒë³µëœ representation ì˜ ì¶œë ¥ í¬ê¸° ë¹„ìœ¨
* $\hat{D}$: output feature dimension
![[Pasted image 20240509203122.png|400]]
#### 1. Read stage

$$
\text{Read} : \mathbb{R}^{N_p+1 \times D} \rightarrow \mathbb{R}^{N_p \times D}
$$
* $Read$ ì—°ì‚°ì€ readout token ì´ í¬í•¨ëœ token ë“¤ê³¼ output dimension ì„ ë§ì¶°ì£¼ëŠ” ì—­í• 
	* ì´ ê³¼ì •ì„ í†µí•´ output ê²°ê³¼ì™€ concat ì„ ê°€ëŠ¥í•˜ë„ë¡í•¨

* Readout token ì˜ ê²½ìš° dense prediction ì‘ì—…ì— ëª…í™•í•œ ëª©ì ì„ ì œê³µí•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.
* í•˜ì§€ë§Œ global information ì„ í¬ì°©í•˜ê³  ì´ë¥¼ ê° token ì— ë¶„ë°°í•˜ëŠ”ë° ë„ì›€ì„ ì¤„ ìˆ˜ ìˆë‹¤.
* $Read$ ì—°ì‚°ì€ ì•„ë˜ 3ê°€ì§€ ì¢…ë¥˜ê°€ ìˆë‹¤.
	1. $\text{Read}_{\text{ignore}}(t) = \{t_1, \dots, t_{N_p}\}$ 
		* ë‹¨ìˆœí•˜ê²Œ readout token ì„ ë¬´ì‹œí•˜ëŠ” ê²½ìš°
	2. $\text{Read}_{\text{add}}(t) = \{t_1 + t_0, \dots, t_{N_p} + t_0\}$ 
		* readout token ì„ ë‹¤ë¥¸ ëª¨ë“  token ì— ê³¨ê³ ë£¨ ë”í•´ì£¼ëŠ” ê²½ìš°
	3. $\text{Read}_{\text{proj}}(t) = \{\text{mlp}(\text{cat}(t_1, t_0)), \dots, \text{mlp}(\text{cat}(t_{N_p}, t_0))\}$ 
		* readout token ê³¼ ì¼ë°˜ token ì„ concat í•œë‹¤.
		* linear layer->GELU ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›ë˜ ì°¨ì› $D$ ë¡œ projection í•œë‹¤.
#### 2. Concatenate stage
* $Read$ ê³¼ì • ì´í›„ $N_p$ ê°œ ì˜ token ì„ positional embedding ì •ë³´ë¥¼ ì´ìš©í•˜ì—¬ ì›ìƒíƒœë¡œ ë°°ì¹˜
* ì´ ê³¼ì •ì„ í†µí•´ $\frac{H}{p} \times \frac{W}{p} \times D$ ì˜ feature representation ì„ ì–»ëŠ”ë‹¤.
$$
\text{Concatenate} : \mathbb{R}^{N_p \times D} \rightarrow \mathbb{R}^{\frac{H}{p} \times \frac{W}{p} \times D}
$$

#### 3. Resample Stage
![[Pasted image 20240510111251.png|500]]
* $Concatenate$ ê³¼ì • ì´í›„ output size $\hat{D}$ ë¥¼ ë§ì¶”ê¸° ìœ„í•´ ì•„ë˜ ê³¼ì •ì„ ê±°ì¹œë‹¤.
	* <mark style='background:var(--mk-color-purple)'>ì•„ë§ˆ ì—¬ê¸°ì„œ ë§í•˜ëŠ” output size ëŠ” semantic segmentation ì˜ semantic map ì˜ channel ì´ë‚˜, depth estimation ì˜ depth map ì˜ channel ì¸ ë“¯ í•œë° í™•ì¸ í•„ìš”</mark>
$$
\text{Resample}_s : \mathbb{R}^{\frac{H}{p} \times \frac{W}{p} \times D} \rightarrow \mathbb{R}^{\frac{H}{s} \times \frac{W}{s} \times \hat{D}}
$$
* 1x1 conv ë¥¼ í†µí•´ input representation $D$ ë¥¼ $\hat{D}$ ë¡œ projection
* $s\geq p$ : stride ê°€ ìˆëŠ” 3x3 conv
* $s<p$ : stride ê°€ ìˆëŠ” 3x3 transpose conv

![[Pasted image 20240510142857.png|500]]
* ìœ„ì˜ Fusion ê³¼ì •ì€ 3ì¢…ë¥˜ì˜ ViT backbone ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì ìš©ëœë‹¤.
	1. ViT-Base: $l=\{3,6,9,12\}$
	2. ViT-Large: $l=\{5,12,18,24\}$
	3. ViT-Hybrid: $l=\{9,12\}$
* ìœ„ì˜ fusion ì´í›„ì˜ feature map ì˜ ì°¨ì› $\hat{D}=256$ ìœ¼ë¡œ ìƒì„±

## Handling varying image sizes
* DPT ëŠ” ì—¬ëŸ¬ í¬ê¸°ì˜ ì´ë¯¸ì§€ë¥¼ ë‹¤ë£° ìˆ˜ ìˆë‹¤.
* image size ê°€ ë‹¬ë¼ì§€ë”ë¼ë„ $p$ ë¡œ ë‚˜ëˆŒìˆ˜ë§Œ ìˆë‹¤ë©´ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•˜ë‹¤.
* í•˜ì§€ë§Œ positional embedding ì€ ìœ„ì¹˜ ì •ë³´ë¥¼ í¬í•¨í•˜ê¸°ì— ì´ë¯¸ì§€ì˜ í¬ê¸°ì— ì˜í–¥ì„ ë°›ëŠ”ë‹¤.
* ì´ëŠ” bilinear interpolation ì„ í†µí•´ ì ì ˆí•œ í¬ê¸°ë¡œ ë³€í˜•í•¨ìœ¼ë¡œì¨ í•´ê²°í•œë‹¤.

# 4. Experiment
* DPT ë¥¼ ì•„ë˜ 2ê°€ì§€ dense prediction ì— ì ìš©í•œë‹¤.
	1. monocular depth estimation
	2. semantic segmentation
* ì´ ë•Œ, DPT ì™€ ë¹„ìŠ·í•œ ìš©ëŸ‰ì„ ê°€ì§€ëŠ” convolution ê³¼ ë¹„êµí•˜ì˜€ì„ ë•Œ, large-scale dataset ì„ ì ìš©í•˜ë©´ DPT ëŠ” convolution ì— ë¹„í•´ ë” í° ì„±ëŠ¥í–¥ìƒì´ ìˆë‹¤.
* ë‹¨ì•ˆ ê¹Šì´ ì¶”ì • ì‘ì—…ì—ì„œëŠ” <mark style='background:var(--mk-color-teal)'>DPT</mark>ê°€ ê¸°ì¡´ì˜ ìµœê³  ì„±ëŠ¥ì„ ë‹¬ì„±í•œ convolutional networkì— ë¹„í•´ <span style='color:var(--mk-color-teal)'>ê¹Šì´ ì˜ˆì¸¡ì˜ ì •í™•ë„ì™€ ì„¸ë°€ë„ì—ì„œ í˜„ì €í•œ ê°œì„ </span>ì„ ë³´ì—¬ì¤€ë‹¤. 
* ì˜ë¯¸ë¡ ì  ë¶„í• ì—ì„œë„ <mark style='background:var(--mk-color-teal)'>DPT</mark>ëŠ” íŠ¹íˆ <span style='color:var(--mk-color-teal)'>ë³µì¡í•œ ì¥ë©´ì—ì„œì˜ ë ˆì´ë¸” ë¶„í• ì˜ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ë° í° íš¨ê³¼</span>ë¥¼ ë‚˜íƒ€ëƒˆë‹¤.


## 4.1. Monocular Depth Estimation
* monocular depth estimation ì€ dense regression task ì´ë‹¤.

### Experiment protocol.
* Loss: [ì´ ë…¼ë¬¸](https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_MegaDepth_Learning_Single-View_CVPR_2018_paper.pdf) ì˜ loss term ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
(ZhengqiLiandNoahSnavely.MegaDepth:Learning single-view depth prediction from Internet photos. In CVPR, 2018.)
* Dataset:
	* ê¸°ì¡´ì— ì‚¬ìš©ë˜ë˜ dataset ë“¤: MIX 5 ë¼ ì§€ì¹­
	* MIX 6 : 5ê°œì˜ ì¶”ê°€ ë°ì´í„°ì…‹ì„ í¬í•¨í•œ meta dataset
		* ì•½ 140ë§Œ ê°œì˜ image
		* í˜„ì¡´í•˜ëŠ” ë°ì´í„°ì…‹ ì¤‘ ê°€ì¥ í¼
* Optimizer: Adam
* Learning rate: 
	* backbone: 1e-5
	* decoder weight: 1e-4
* crop
	* ì´ë¯¸ì§€ì˜ ê°€ì¥ ê¸´ ë©´ì´ 384 pixel ì´ ë˜ë„ë¡ í¬ê¸°ë¥¼ ì¡°ì •
	* 384 x 384 í¬ê¸°ì˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©
* 72000 step, 16 batch size per 1 epoch, 60 epoch

### Zero-shot cross-dataset transfer
![[Pasted image 20240510150130.png]]
* Table 1 ì€ í›ˆë ¨ ì¤‘ì— ë³´ì§€ ëª»í•œ ë‹¤ë¥¸ dataset ìœ¼ë¡œ zero-shot transfer ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤.
(evaluation metric ì€ [ì´ ë…¼ë¬¸](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9178977)ì—ì„œ ì‚¬ìš©ëœ metric ì„ ì‚¬ìš©í•˜ì˜€ëŠ”ë° ë„£ì„ì§€ë§ì§€ ê³ ë¯¼ì¤‘)
([30] Rene Ì Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. TPAMI, 2020.)
* ìœ„ì˜ 6ê°œì˜ metric ì— ëŒ€í•´ ëª¨ë‘ SOTA ë‹¬ì„±(6ê°œì˜ metric ëª¨ë‘ ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ìˆ˜ì¹˜)
* DPT-Hybrid ì˜ ê²½ìš° ê¸°ì¡´ì˜ SOTA ì™€ ë¹„ìŠ·í•œ ìš©ëŸ‰ê³¼ inference time ìœ¼ë¡œ SOTA ë‹¬ì„±
* DPT-Large ì˜ ê²½ìš° ê¸°ì¡´ì˜ SOTA ì— ë¹„í•´ 3ë°°ì˜ ìš©ëŸ‰ì„ ê°€ì§€ë©°, inference time ì€ ë¹„ìŠ·
![[Pasted image 20240510150945.png|500]]
* ë˜í•œ ì´ ë…¼ë¬¸ì—ì„œëŠ” DPT ê°€ large-scale dataset ë•Œë¬¸ë§Œì€ ì•„ë‹Œì§€ íŒë‹¨í•˜ê¸° ìœ„í•´ MiDaS model ë„ MIX6 ì—ì„œ ì¬í›ˆë ¨í•˜ì—¬ í‰ê°€ë¥¼ ì§„í–‰í•˜ì˜€ë‹¤.
* large-scale dataset ì€ MiDaS model ì— í™•ì‹¤í•œ ì´ì ì„ ì œê³µí•˜ì§€ë§Œ DPT ê°€ ì—¬ì „íˆ SOTA ì„±ëŠ¥ì´ë‹¤.
![[Pasted image 20240510151322.png|500]]
* Figure 2 ì—ì„œë„ ë³¼ ìˆ˜ ìˆë“¯, DPT ëŠ” MiDaS ì´ ë¹„í•´ detail ì„ ë” ì˜ ì¡ì„ ìˆ˜ ìˆë‹¤.

## 4.2. Semantic Segmentation

* semantic segmentationì€ discrete labeling ì‘ì—…ì„ ëŒ€í‘œí•˜ë©°, dense prediction ì•„í‚¤í…ì²˜ë¥¼ ê²€ì¦í•˜ëŠ” ë§¤ìš° ê²½ìŸì ì¸ ë¶„ì•¼ì´ë‹¤. 
* ì´ì „ ì‹¤í—˜ê³¼ ë™ì¼í•œ ë°±ë³¸ê³¼ ë””ì½”ë” êµ¬ì¡°ë¥¼ ì‚¬ìš©í•œë‹¤. 
* ì´ ë…¼ë¬¸ì—ì„œëŠ” half resolutionì—ì„œ ì˜ˆì¸¡í•˜ê³  bilinear interpolationì„ ì‚¬ìš©í•˜ì—¬ ë¡œì§“ì„ ì „ì²´ í•´ìƒë„ë¡œ ì—…ìƒ˜í”Œë§í•˜ëŠ” ì¶œë ¥ í—¤ë“œë¥¼ ì‚¬ìš©í•œë‹¤. 
* ì¸ì½”ë”ëŠ” ë‹¤ì‹œ ImageNetì—ì„œ ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ë¡œ ì´ˆê¸°í™”ë˜ê³ , ë””ì½”ë”ëŠ” ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ëœë‹¤.

### Experiment protocol.
* [ì´ ë…¼ë¬¸](https://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Zhang_ResNeSt_Split-Attention_Networks_CVPRW_2022_paper.pdf) ì˜ protocol ì„ ë°€ì ‘í•˜ê²Œ ë”°ë¥¸ë‹¤
[51] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi Zhang, Haibin Lin, Yue Sun, Tong He, Jonas Muller, R. Manmatha, Mu Li, and Alexander Smola. ResNeSt: Split- attention networks. arXiv preprint arXiv:2004.08955, 2020.
* loss: cross-entropy loss
* metric: pixACC(pixel ë³„ ì •í™•ë„), mIoU

### ADE20K
* DPT ë¥¼ í•´ë‹¹ dataset ì— ëŒ€í•´ 240 epoch ë™ì•ˆ í•™ìŠµ
* DPT-Hybrid ëŠ” ê¸°ì¡´ì˜ fully convolutional architecture ë¥¼ ëŠ¥ê°€í•œë‹¤.
* DPT-Large ì—ì„œëŠ” ì˜¤íˆë ¤ ì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤.
	* ì´ëŠ” DPT-Large ê°€ í•™ìŠµí•œ ë°ì´í„°ì…‹ë³´ë‹¤ ADE20K ì˜ ê·œëª¨ê°€ ë„ˆë¬´ ì ê¸°ì— ë°œìƒí•œ ë¬¸ì œì´ë‹¤.
![[Pasted image 20240510154013.png|500]]
![[Pasted image 20240510154106.png]]
* Figure 3ì—ì„œë„ ì•Œ ìˆ˜ ìˆë“¯ DPT ê°€ ë” ì„¸ë°€í•œ segment ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤.

## 4.3. Ablations
* Ablation sutdy ì—ì„œëŠ” monocular depth estimation ì„ task ë¡œ ì¡ëŠ”ë‹¤.
* ê¸°ì¡´ì˜ MIX 6 dataset ì´ ì•„ë‹Œ 3ê°œì˜ dataset ì— ëŒ€í•œ ì¶•ì†Œëœ meta dataset ìœ¼ë¡œ ablation study ë¥¼ ì§„í–‰í•¨
	* HRWSI
	* BlendedMVS
	* ReDWeb
* ìœ„ì˜ 3ê°œì˜ dataset ì€ ë¹„êµì  ê³ í’ˆì§ˆì˜ ì •ë°€í•œ ground truth ë¥¼ ì œê³µí•˜ê¸°ì— ì±„íƒë˜ì—ˆë‹¤.
* ì•„ë˜ experiment ì—ì„œ íŠ¹ë³„íˆ ëª…ì‹œë˜ì§€ ì•ŠëŠ” í•œ, backbone ìœ¼ë¡œ ViT-Base ë¥¼ ì‚¬ìš©í•œë‹¤.

### Skip Connections.
![[Pasted image 20240510154844.png|500]]
* Base:
	* ê²°ê³¼ì—ì„œë„ ë³¼ ìˆ˜ ìˆë“¯, high-level feature ë§Œ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ low-level ê³¼ high-level ì„ ê°™ì´ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” íš¨ê³¼ì 
* Hybrid:
	* $R0,R1$: ResNet50 embedding network ì˜ ì²«ë²ˆì§¸ ë° ë‘ ë²ˆì§¸ downsampling ë‹¨ê³„(layer)ì—ì„œ ì¶”ì¶œëœ feature ë¥¼ í™œìš©í•˜ëŠ” ê²ƒ.
	* ì´ ë¶€ë¶„ ì—­ì‹œ high-level, low-level feature ë¥¼ ë™ì‹œì— í™œìš©í•˜ëŠ” ê²ƒì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆìŒì„ ëª…ì‹œí•œë‹¤.

### Readout token
* Reassemble block ì˜ ì²« ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•˜ëŠ” readout token(for classification) ì— ëŒ€í•œ ì—°êµ¬
![[Pasted image 20240510155257.png|500]]
* token ì„ ë¬´ì‹œí•˜ëŠ” ë°©ì‹($Ignore$)ì´ ì¼ë¶€ ë°ì´í„°ì…‹ì—ì„œëŠ” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ $Project$ í•˜ëŠ” ê²ƒì´ í‰ê·  ì„±ëŠ¥ì€ ë” ì¢‹ë‹¤.

### Backbones.
![[Pasted image 20240510155507.png|500]]
* ViT-Large ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì€ ê°€ì¥ ì¢‹ë‹¤(í‰ê·  ê°’ì´ ë†’ìŒ)
* í•˜ì§€ë§Œ ViT-Large ëŠ” ViT-Base ë‚˜ ViT-Hybrid ì— ë¹„í•´ 3ë°°ì˜ parameter ë¥¼ ê°€ì§„ë‹¤.
* ë”°ë¼ì„œ ViT-Hybrid ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ê°€ì¥ balance ê°€ ì¢‹ë‹¤.

### Inference resolution.
* convolution ì˜ ê²½ìš° ê°™ì€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì˜€ì„ ë•Œ input image ì˜ resolution ì´ ì»¤ì§€ë©´ input image ì— ëŒ€í•œ receptive field ì˜ ë¹„ìœ¨ì´ ì‘ì•„ì§„ë‹¤.
* ì´ëŠ” input image ì˜ resolution ì´ ì»¤ì§ˆìˆ˜ë¡ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” ê²°ê³¼ë¥¼ ë‚³ëŠ”ë‹¤.
* í•˜ì§€ë§Œ DPT ëŠ” ë‹¤ë¥¸ ëª¨ë¸ì— ë¹„í•´ì„œ resolution ì´ ì»¤ì§ˆìˆ˜ë¡ ì„±ëŠ¥í•˜ë½ì˜ í­ì´ ì ë‹¤.
* ì´ëŠ” DPT ê°€ resolution ì— ëœ ì˜ì¡´ì ì´ë¼ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.
![[Pasted image 20240510161142.png|500]]

### Inference speed.
* DPT-Hybrid ì™€ DPT-Large ëŠ” MiDaS ì—ì„œ ì‚¬ìš©í•˜ëŠ” fully convolutional architecture ì™€ ìœ ì‚¬í•œ latency ë¥¼ ë³´ì¸ë‹¤.
* DPT-Large ì˜ ê²½ìš° ë‹¤ë¥¸ architecture ì— ë¹„í•´ ë†’ì€ memory ë¥¼ ìš”êµ¬í•˜ì§€ë§Œ ë†’ì€ ë³‘ë ¬ì„±ì„ í†µí•´ ë‹¤ë¥¸ ëª¨ë¸ê³¼ ë¹„ìŠ·í•œ latency ë¥¼ ë³´ì¸ë‹¤.
