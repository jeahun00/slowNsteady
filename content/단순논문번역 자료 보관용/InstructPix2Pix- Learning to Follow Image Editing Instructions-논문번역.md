# Abstract

우리는 사람의 지시에 따라 이미지를 편집하는 방법을 제안합니다: 입력 이미지와 모델에게 무엇을 해야 할지 알려주는 written 지시를 주면, 우리의 모델은 이 지시를 따라 이미지를 편집합니다. 이 문제에 대한 훈련 데이터를 얻기 위해, 두 개의 큰 사전 훈련된 모델인 언어 모델(GPT-3)과 텍스트-이미지 모델(Stable Diffusion)의 지식을 결합하여 대규모 이미지 편집 예시 데이터셋을 생성합니다. 우리의 conditional diffusion 모델인 InstructPix2Pix은 생성된 데이터에 훈련되어 실제 이미지와 사용자가 작성한 지시에 대해서도 일반화되며, 추론 시간에 이미지를 편집합니다. 우리의 모델은 전방향 패스에서 편집을 수행하며, 예시별로 미세 조정이나 inversion을 요구하지 않기 때문에 몇 초 내에 이미지를 빠르게 편집합니다. 다양한 입력 이미지와 written 지시에 대해 설득력 있는 편집 결과를 보여줍니다.


# 1. Introduction

우리는 이미지 편집을 위한 인간이 작성한 지시를 따를 수 있는 생성 모델을 교육하는 방법을 제시합니다. 이 작업을 위한 훈련 데이터를 대규모로 확보하는 것은 어렵기 때문에, 다양한 모달리티에서 사전 훈련된 여러 대형 모델을 결합한 쌍 데이터셋을 생성하는 접근 방식을 제안합니다: 대형 언어 모델(GPT-3 [7])과 텍스트-이미지 모델(Stable Diffusion [51]). 이 두 모델은 언어와 이미지에 대한 보완적인 지식을 포착하여 두 모달리티를 아우르는 작업을 위한 쌍 훈련 데이터를 생성하는 데 결합될 수 있습니다.

우리가 생성한 쌍 데이터를 사용하여, 입력 이미지와 그것을 편집하는 방법에 대한 텍스트 지시가 주어졌을 때 편집된 이미지를 생성하는 conditional diffusion 모델을 훈련합니다. 우리의 모델은 전방향 패스에서 직접 이미지 편집을 수행하며, 추가적인 예시 이미지, 입력/출력 이미지의 전체 설명, 또는 예시별 미세 조정을 요구하지 않습니다. 전적으로 합성 예시(즉, 생성된 written 지시와 생성된 이미지)에 대해 훈련되었음에도 불구하고, 우리의 모델은 임의의 실제 이미지와 자연스러운 인간 작성 지시에 대해 제로-샷 일반화를 달성합니다. 우리의 모델은 객체 교체, 이미지의 스타일 변경, 설정 변경, 예술 매체 변경 등 다양한 편집을 수행할 수 있는 인간의 지시를 따를 수 있는 직관적인 이미지 편집을 가능하게 합니다. 선택된 예시는 그림 1에서 찾을 수 있습니다.

# 2. Prior Work
**Composing large pretrained models** 
최근 연구에서는 대형 사전 훈련된 모델들을 조합하여 단일 모델로는 수행할 수 없는 다모달 작업, 예를 들어 이미지 캡셔닝과 시각적 질문 답변(언어 모델과 텍스트-이미지 모델 모두의 지식이 요구되는 작업)을 해결할 수 있음을 보여주었습니다. 사전 훈련된 모델들을 결합하는 기법에는 새로운 작업에 대한 공동 미세조정[4, 33, 40, 67], 프롬프트를 통한 의사소통[62, 69], 에너지 기반 모델의 확률 분포 조합[11, 37], 다른 모델로부터의 피드백으로 하나의 모델을 안내하는 방법[61], 그리고 반복적 최적화[34]가 있습니다. 우리의 방법은 GPT-3 [7]과 Stable Diffusion [51]이라는 두 개의 사전 훈련된 모델의 보완적 능력을 활용한다는 점에서 이전 작업과 유사하지만, 이 모델들을 사용하여 쌍을 이루는 다모달 훈련 데이터를 생성한다는 점에서 차이가 있습니다.

**Diffusion-based generative models** 
최근 diffusion 모델에서의 진보[59]는 최첨단 이미지 합성[10, 18, 19, 53, 55, 60] 뿐만 아니라 비디오[21, 58], 오디오[30], 텍스트[35], 네트워크 매개변수[45] 등 다른 모달리티의 생성 모델을 가능하게 했습니다. 최근의 텍스트-이미지 diffusion 모델들[41, 48, 51, 54]은 임의의 텍스트 캡션에서 사실적인 이미지를 생성할 수 있음을 보여주었습니다.

**Generative models for image editing** 
이미지 편집 모델은 전통적으로 스타일 전송[15, 16]이나 이미지 도메인 간 변환[22, 24, 36, 42, 71]과 같은 단일 편집 작업을 대상으로 했습니다. 수많은 편집 접근 방식은 이미지를 latent 공간(예: StyleGAN[25, 26])으로 역전[1–3, 12] 또는 인코딩[8, 50, 63]하여 latent 벡터를 조작함으로써 편집할 수 있습니다. 최근 모델들은 CLIP[47] 임베딩을 사용하여 텍스트를 사용한 이미지 편집을 안내하는 데 활용되었습니다[5,9,14,28,31,41,44,70]. 우리는 이러한 방법 중 하나인 Text2Live[6]와 비교하는데, 이는 CLIP 유사성 목표를 최대화하는 추가 이미지 레이어를 최적화하는 편집 방법입니다.

최근 연구에서는 텍스트-이미지 diffusion 모델을 이미지 편집에 사용했습니다[5,17,27,38,48]. 일부 텍스트-이미지 모델은 원래 이미지를 편집할 수 있는 능력을 가지고 있습니다(예: DALLE-2는 이미지의 변형을 생성하고, 영역을 인페인팅하며, CLIP 임베딩을 조작할 수 있습니다[48]), 하지만 이러한 모델들을 대상으로 한 편집에 사용하는 것은 간단하지 않습니다. 대부분의 경우 유사한 텍스트 프롬프트가 유사한 이미지를 생성한다는 보장이 없기 때문입니다. 최근 Hertz 등[17]의 연구는 유사한 텍스트 프롬프트에 대해 생성된 이미지를 흡수하는 Prompt-to-Prompt 방법을 통해 이 문제를 다루었습니다. 이 방법은 생성된 이미지에 대해 고립된 편집을 할 수 있도록 합니다. 우리는 이 방법을 훈련 데이터 생성에 사용합니다. 비생성(즉, 실제) 이미지를 편집하기 위해, SDEdit[38]은 사전 훈련된 모델을 사용하여 입력 이미지에 잡음을 추가하고 새로운 대상 프롬프트로 제거합니다. 우리는 SDEdit을 기준선으로 비교합니다. 다른 최근 작업들은 주어진 캡션과 사용자가 그린 마스크를 기반으로 지역적 인페인팅을 수행하거나[5, 48], 소수의 이미지에서 배운 특정 객체나 개념의 새로운 이미지를 생성하거나[13, 52], 단일 이미지를 역전시키고(미세 조정한 후) 새로운 텍스트 설명으로 재생성하여 편집을 수행합니다[27]. 이러한 접근 방식과는 대조적으로, 우리의 모델은 단일 이미지와 그 이미지를 편집하는 방법에 대한 지시(즉, 어떠한 이미지의 전체 설명도 아님)만을 받아 전방향 패스에서 직접 편집을 수행하며, 사용자가 그린 마스크, 추가 이미지, 또는 예시별 역전이나 미세 조정이 필요하지 않습니다.

**Learning to follow instructions**
우리의 방법은 모델에게 수행할 행동을 알려주는 지시를 통해 편집을 가능하게 한다는 점에서 기존의 텍스트 기반 이미지 편집 작업들[6, 13, 17, 27, 38, 52]과 차별화됩니다. 이는 입력/출력 이미지의 텍스트 라벨, 캡션 또는 설명과는 대조적입니다. 편집 지시를 따르는 주요 이점은 사용자가 자연스러운 written 텍스트로 모델에게 정확히 무엇을 해야 할지 알릴 수 있다는 것입니다. 사용자가 추가 정보를 제공할 필요가 없습니다. 예를 들어, 입력과 출력 이미지 사이에서 일정하게 유지되는 시각적 내용의 예시 이미지나 설명 같은 것들이 필요하지 않습니다. 지시는 표현력이 풍부하고 정확하며 직관적으로 작성할 수 있어, 사용자가 특정 객체나 시각적 속성의 변경을 쉽게 분리할 수 있습니다. 우리의 목표는 언어 작업을 위해 인간의 지시를 더 잘 따르도록 대형 언어 모델을 교육하는 최근의 작업[39, 43, 68]에 영감을 받아 written 이미지 편집 지시를 따르는 것입니다.

**Training data generation with generative models**
심층 모델은 일반적으로 대량의 훈련 데이터를 요구합니다. 인터넷 데이터 컬렉션은 종종 적합하지만, 감독을 위해 필요한 형태, 예를 들어 특정 모달리티의 쌍 데이터가 존재하지 않을 수 있습니다. 생성 모델이 계속해서 개선됨에 따라, 이들을 하류 작업을 위한 저렴하고 풍부한 훈련 데이터의 원천으로 사용하는 데 대한 관심이 증가하고 있습니다[32, 46, 49, 57, 64, 65]. 이 논문에서는 두 가지 다른 형태의 상용 생성 모델(언어, 텍스트-이미지)을 사용하여 우리의 편집 모델을 위한 훈련 데이터를 생산합니다.

# 3. Method

우리는 지시 기반 이미지 편집을 감독 학습 문제로 취급합니다: (1) 먼저, 편집 전후의 이미지와 텍스트 편집 지시로 구성된 쌍 훈련 데이터셋을 생성합니다(3.1절, 그림 2a-c 참조), 그리고 (2) 이 생성된 데이터셋에 대해 이미지 편집 diffusion 모델을 훈련합니다(3.2절, 그림 2d 참조). 생성된 이미지와 편집 지시로 훈련되었음에도 불구하고, 우리 모델은 임의의 인간이 작성한 지시를 사용하여 실제 이미지를 편집하는 데 일반화할 수 있습니다. 우리 방법의 개요는 그림 2를 참조하십시오.

## 3.1. Generating a Multi-modal Training Dataset
우리는 서로 다른 모달리티에서 작동하는 두 개의 대규모 사전 훈련된 모델—큰 언어 모델[7]과 텍스트-이미지 모델[51]—의 능력을 결합하여, 텍스트 편집 지시와 해당 편집 전후의 이미지를 포함하는 다모달 훈련 데이터셋을 생성합니다. 다음 두 절에서 이 과정의 두 단계를 자세히 설명합니다. 3.1.1절에서는 GPT-3[7]을 미세 조정하여 텍스트 편집 모음을 생성하는 과정을 설명합니다: 이미지를 설명하는 프롬프트가 주어지면, 변경될 내용을 설명하는 텍스트 지시와 그 변경 후 이미지를 설명하는 프롬프트를 생성합니다(그림 2a). 그 다음, 3.1.2절에서는 편집 전후의 두 텍스트 프롬프트를 텍스트-이미지 모델[51]을 사용하여 해당 이미지 쌍으로 변환하는 과정을 설명합니다(그림 2b).

### 3.1.1 Generating Instructions and Paired Captions
우리는 먼저 텍스트 영역에서 작업을 시작하며, 여기서 대규모 언어 모델을 활용하여 이미지 캡션을 입력받고 편집 지시 및 편집 후 결과적인 텍스트 캡션을 생성합니다. 예를 들어, 그림 2a에서 보여주듯이 입력 캡션 "소녀가 말을 타는 사진"이 주어지면, 우리의 언어 모델은 "소녀가 용을 타게 하라"라는 타당한 편집 지시와 적절하게 수정된 출력 캡션 "소녀가 용을 타는 사진"을 생성할 수 있습니다. 텍스트 영역에서 작업함으로써, 우리는 이미지 변화와 텍스트 지시 사이의 일치를 유지하면서 다양하고 방대한 편집 모음을 생성할 수 있습니다.

우리의 모델은 입력 캡션, 편집 지시, 출력 캡션으로 구성된 비교적 작은 인간 작성 데이터셋에 GPT-3를 미세 조정함으로써 훈련됩니다. 미세 조정 데이터셋을 생성하기 위해 LAION-Aesthetics V2 6.5+ [56] 데이터셋에서 700개의 입력 캡션을 샘플링하고 수동으로 지시와 출력 캡션을 작성했습니다. 우리가 작성한 지시와 출력 캡션의 예는 표 1a에서 볼 수 있습니다. 이 데이터를 사용하여 GPT-3 Davinci 모델을 기본 훈련 매개변수를 사용하여 단일 에폭 동안 미세 조정했습니다.

GPT-3의 방대한 지식과 일반화 능력으로 인해, 우리의 미세 조정된 모델은 창의적이면서도 합리적인 지시와 캡션을 생성할 수 있습니다. GPT-3이 생성한 데이터 예는 표 1b에서 볼 수 있습니다. 우리의 데이터셋은 이 훈련된 모델을 사용하여 다수의 편집과 출력 캡션을 생성함으로써 만들어졌으며, 입력 캡션은 LAION-Aesthetics의 실제 이미지 캡션에서 가져옵니다(중복 캡션 또는 중복 이미지 URL을 제외). 우리는 그 크기가 크고, 내용의 다양성(적절한 명사와 대중 문화 참조 포함), 다양한 매체(사진, 그림, 디지털 아트워크) 때문에 LAION 데이터셋을 선택했습니다. LAION의 latent적인 단점은 매우 잡음이 많고 비합리적이거나 기술적이지 않은 캡션을 다수 포함한다는 것입니다—하지만, 데이터셋 필터링(3.1.2절)과 classifier free guidance(3.2.1절)의 조합을 통해 데이터셋의 잡음이 완화되는 것을 발견했습니다. 최종적으로 생성된 지시와 캡션으로 구성된 우리의 말뭉치는 454,445개의 예를 포함합니다.

### 3.1.2 Generating Paired Images from Paired Captions
다음으로, 사전 훈련된 텍스트-이미지 모델을 사용하여 편집 전후를 나타내는 자막 쌍을 이미지 쌍으로 변환합니다. 자막 쌍을 해당 이미지 쌍으로 변환하는 과정에서 한 가지 도전은 텍스트-이미지 모델이 conditional 프롬프트의 아주 사소한 변경에도 이미지 일관성에 대한 보장을 제공하지 않는다는 것입니다. 예를 들어, "고양이의 사진"과 "검은 고양이의 사진"과 같은 매우 유사한 두 프롬프트는 완전히 다른 고양이 이미지를 생성할 수 있습니다. 이는 우리의 목적에 부적합합니다. 우리는 이 쌍 데이터를 사용하여 모델이 이미지를 편집하도록 훈련하는 데 감독으로 사용하려고 합니다(다른 임의의 이미지를 생성하지 않음). 따라서 우리는 Prompt-to-Prompt[17]를 사용합니다. 이는 최근에 개발된 방법으로, 텍스트-이미지 diffusion 모델에서 여러 생성물이 유사하도록 장려하는 것을 목표로 합니다. 이는 일부 잡음 제거 단계에서 빌려온 교차 주의 가중치를 통해 이루어집니다. 그림 3은 Prompt-to-Prompt 사용 유무에 따른 샘플 이미지를 비교합니다.

이 방법은 생성된 이미지를 동화하는 데 큰 도움이 되지만, 다른 편집은 이미지 공간에서 다른 양의 변경을 요구할 수 있습니다. 예를 들어, 대규모 이미지 구조를 변경하는 큰 크기의 변경(예: 객체 이동, 다른 모양의 객체로 교체)은 생성된 이미지 쌍 간의 유사성이 덜 필요할 수 있습니다. 다행히도, Prompt-to-Prompt는 두 이미지 간의 유사성을 제어할 수 있는 매개변수를 가지고 있습니다: 공유된 주의 가중치를 가진 잡음 제거 단계의 비율 p입니다. 불행히도, 자막과 편집 텍스트만으로 p의 최적값을 식별하는 것은 어렵습니다. 따라서 우리는 각 자막 쌍마다 100개의 이미지 쌍 샘플을 생성하며, 각각 $p \sim \mathcal{U}(0.1, 0.9)$의 무작위 값을 가지고, 이 샘플들을 CLIP 기반 메트릭을 사용하여 필터링합니다: Gal 등[14]에 의해 도입된 CLIP 공간에서의 방향 유사성입니다. 이 메트릭은 두 이미지 간의 변화와 두 이미지 캡션 간의 변화 사이의 일관성을 측정합니다. 이 필터링을 수행함으로써 우리의 이미지 쌍의 다양성과 품질을 최대화할 뿐만 아니라, 우리의 데이터 생성을 Prompt-to-Prompt와 Stable Diffusion의 실패에 더 강건하게 만듭니다.

## 3.2. InstructPix2Pix
우리는 생성된 훈련 데이터를 사용하여 written 지시에서 이미지를 편집하는 conditional diffusion 모델을 훈련합니다. 이 모델은 대규모 텍스트-이미지 latent diffusion 모델인 Stable Diffusion을 기반으로 합니다.

diffusion 모델[59]은 데이터 샘플을 노이즈가 없는 autoencoder 시퀀스를 통해 생성하는 것을 배웁니다. 이 시퀀스는 데이터 분포의 점수[23]를 추정합니다(밀도가 높은 데이터를 향하는 방향). latent diffusion[51]은 사전 훈련된 variational autoencoder[29]의 latent 공간에서 운영함으로써 diffusion 모델의 효율성과 품질을 개선합니다. 이 autoencoder는 인코더 $\mathcal{E}$ 와 디코더 $\mathcal{D}$를 가지고 있습니다. 이미지 $x$에 대해, diffusion 과정은 인코딩된 latent 변수 $z = \mathcal{E}(x)$에 노이즈를 추가하여 노이즈가 있는 latent 변수 $z_t$를 생성합니다. 여기서 노이즈 수준은 시간스텝 $t$에 따라 증가합니다. 우리는 이미지 조건 $c_I$과 텍스트 지시 조건 $c_T$을 주어진 latent 노이즈 $z_t$에 더해진 노이즈를 예측하는 네트워크 $\theta$를 배웁니다. 우리는 다음 latent diffusion 목표를 최소화합니다:

$$L = \mathbb{E}_{(x, c_I, c_T), t \sim \mathcal{N}(0,1), t \in T} \left[ \left\| z - \theta(z_t, t, \mathcal{E}(c_I), c_T) \right\|^2 \right] \quad (1)$$

Wang et al. [66]은 이미지 변환 작업에 대해, 특히 쌍 훈련 데이터가 제한적일 때, 대규모 이미지 diffusion 모델을 미세 조정하는 것이 처음부터 모델을 훈련하는 것보다 뛰어난 성능을 보인다고 보고했습니다. 그러므로 우리는 이미지-텍스트 생성 능력이 방대한 사전 훈련된 Stable Diffusion 체크포인트로부터 우리 모델의 가중치를 초기화함으로써 이를 활용합니다. 이미지 조건을 지원하기 위해, 첫 번째 합성곱 계층에 추가적인 입력 채널을 추가하고 $z_t$와 $\mathcal{E}(c_I)$를 연결합니다. diffusion 모델의 모든 사용 가능한 가중치는 사전 훈련된 체크포인트에서 초기화되며, 새롭게 추가된 입력 채널에서 작동하는 가중치는 0으로 초기화됩니다. 원래 캡션을 위해 의도된 같은 텍스트 조건 메커니즘을 재사용하여 대신 텍스트 편집 지시 $c_T$를 입력으로 받습니다. 추가적인 훈련 세부 사항은 보충 자료의 부록 C에 제공됩니다.
### 3.2.1 Classifier-free Guidance for Two Conditionings
classifier free diffusion guidance[20]은 diffusion 모델에 의해 생성된 샘플의 품질과 다양성을 희생하여 보다 더 높은 질의 데이터에 확률 질량을 이동시키는 방법입니다. 이는 일반적으로 클래스 conditional 및 텍스트 conditional 이미지 생성에 사용되어 생성된 이미지의 시각적 품질을 향상시키고, 샘플링된 이미지가 그들의 conditional에 더 잘 일치하도록 합니다. classifier free guidance은 암시적인 classifier $p_\theta (c | z_t)$가 conditional c에 높은 가능성을 할당하는 곳으로 확률 질량을 효과적으로 이동시킵니다. classifier free guidance의 구현은 conditional 및 비conditional 잡음 제거에 대해 diffusion 모델을 공동으로 훈련하고, 추론 시 두 점수 추정치를 결합하는 것을 포함합니다. 비conditional 잡음 제거 훈련은 훈련 중 일정 빈도로 conditional c를 고정된 빈값 $c = \varnothing$으로 설정함으로써 단순히 수행됩니다. 추론 시, guidance 규모 s가 1 이상일 때, 수정된 점수 추정치 $\tilde{e_\theta}(z_t, c)$는 conditional $e_\theta(z_t, c)$ 방향으로 확대되고 비conditional $e_\theta(z_t, \varnothing)$에서 멀어집니다.

$$
\tilde{e_\theta}(z_t, c) = e_\theta(z_t, \varnothing) + s \cdot (e_\theta(z_t, c) - e_\theta(z_t, \varnothing)) (2)
$$

우리의 작업에 있어서, 점수 네트워크 $e_\theta(z_t, c_I, c_T)$는 두 가지 conditional, 즉 입력 이미지 $c_I$와 텍스트 지시 $c_T$에 대해 갖습니다. 우리는 두 conditional 모두에 대하여 classifier free guidance을 활용하는 것이 유익하다는 것을 발견했습니다. Liu et al. [37]은 conditional diffusion 모델이 다양한 conditional 값에서 점수 추정치를 조합할 수 있다는 것을 보여주었습니다. 
우리는 두 개의 별개의 conditional 입력으로 우리 모델에 같은 개념을 적용합니다. 훈련 중, 우리는 5%의 예시에서만 $c_I = \varnothing$으로 무작위로 설정하고, 5%의 예시에서만 $c_T = \varnothing$으로, 그리고 또 다른 5%의 예시에서는 $c_I = \varnothing$과 $c_T = \varnothing$ 둘 다를 설정합니다. 따라서 우리 모델은 두 conditional 또는 둘 중 하나의 conditional에 대한 비conditional 잡음 제거를 할 수 있습니다. 우리는 두 가지 guidance 규모, $s_I$와 $s_T$를 도입합니다. $s_I$를 증가시키면 입력 이미지와 더 밀접하게 닮은 편집된 이미지가 나오고, $s_T$를 증가시키면 더 강렬한 편집이 나타납니다. 수정된 점수 추정치는 다음과 같습니다:

$$
\begin{align*}
\tilde{e_\theta}(z_t, c_I, c_T) = &e_\theta(z_t, \varnothing, \varnothing) \\
&+ s_I \cdot (e_\theta(z_t, c_I, \varnothing) - e_\theta(z_t, \varnothing, \varnothing))\\
&+ s_T \cdot (e_\theta(z_t, c_I, c_T) - e_\theta(z_t, c_I, \varnothing))
\end{align*}
$$

그림 4에서는 이 두 매개변수가 생성된 샘플에 미치는 영향을 보여줍니다. classifier free guidance 공식에 대한 자세한 사항은 보충 자료의 부록 D에서 확인할 수 있습니다.

# 4. Result

우리는 다양한 종류의 실제 사진과 예술 작품에 대해 많은 편집 유형과 지시어의 표현을 사용한 지시 기반 이미지 편집 결과를 보여줍니다. 선택된 결과는 보충 자료의 그림 1, 5, 6, 7, 11, 12 및 부록 A에서 확인할 수 있습니다. 우리 모델은 객체 교체, 계절 및 날씨 변경, 배경 교체, 재질 속성 수정, 예술 매체 변환 등 다양한 어려운 편집 작업을 성공적으로 수행합니다.

우리는 최근의 작업인 SDEdit [38], Text2Live [6], 그리고 Prompt-to-Prompt [17]와 질적으로 비교합니다. 우리 모델은 이미지를 어떻게 편집할지에 대한 지시를 따르지만, 이전 작업들(이 기준선 방법을 포함)은 이미지(또는 편집 레이어)의 설명을 기대합니다. 따라서, 우리는 편집 지시 대신 편집 후 텍스트 캡션을 제공합니다. 또한, 이미지 일관성과 편집 품질을 측정하는 두 가지 메트릭을 사용하여 SDEdit 및 Prompt-to-Prompt와 정량적으로 비교합니다. 이는 섹션 4.1에서 더 자세히 설명되어 있습니다. 마지막으로, 생성된 훈련 데이터의 크기와 품질이 우리 모델의 성능에 어떤 영향을 미치는지에 대한 연구를 섹션 4.2에서 보여줍니다.

## 4.1. 기준선 비교

우리는 SDEdit[38], Text2Live[6], 그리고 Prompt-to-Prompt[17]와 질적인 비교뿐만 아니라 SDEdit과 Prompt-to-Prompt와의 정량적 비교도 제공합니다. SDEdit[38]은 사전 훈련된 diffusion 모델로 이미지를 편집하는 기술로, 부분적으로 잡음이 가해진 이미지를 입력으로 넣고 잡음 제거를 통해 새로운 편집된 이미지를 생성합니다. Text2Live[6]는 텍스트 프롬프트를 조건으로 하는 색상+불투명도 증강 레이어를 생성하여 이미지를 편집합니다.

우리는 그림 9에서 SDEdit, Text2Live 및 Prompt-to-Prompt와 질적인 비교를 합니다. 다른 예시에서의 추가 비교뿐만 아니라 이 관련 작업들의 다른 설정에 대한 비교는 보충 자료의 부록 B에서 제공됩니다. 우리는 SDEdit이 내용이 대략적으로 일정하게 유지되고 스타일이 변경되는 경우에는 합리적으로 잘 작동하지만, 더 큰 변화가 원해질 때 특히 정체성을 유지하고 개별 객체를 분리하는 데 어려움을 겪는다는 것을 알게 되었습니다. 또한, 편집 지시가 아닌 원하는 이미지의 전체 출력 설명을 요구합니다. 한편, Text2Live는 덧셈 레이어를 포함한 편집에 대해 설득력 있는 결과를 제공할 수 있지만, 그 공식은 처리할 수 있는 편집 범주를 제한합니다.

SDEdit 및 Prompt-to-Prompt와의 정량적 비교는 그림 8에서 보여집니다. 우리는 두 가지 지표, CLIP 이미지 임베딩의 코사인 유사도(편집된 이미지가 입력 이미지와 얼마나 일치하는지)와 [14]에 의해 소개된 방향 CLIP 유사도(텍스트 캡션의 변화가 이미지 변화와 얼마나 일치하는지) 사이의 트레이드오프를 그래프로 나타냅니다. 이는 경쟁 지표입니다—원하는 편집과 출력의 대응도를 증가시킬수록 입력 이미지와의 유사도가 감소하게 됩니다—그리고 우리는 어느 방법이 최상의 트레이드오프(가장 높은 곡선)를 달성하는지에 관심이 있습니다. 우리는 SDEdit과 Prompt-to-Prompt에 비해 우리의 결과가 같은 이미지 유사도 값에 대해 더 높은 방향 유사도를 달성함을 발견했으며, 이는 우리가 원하는 편집을 더 잘 수행한다는 것을 나타냅니다. 이러한 발견은 2000개의 편집에 대해 평균적으로 측정되었으며, 우리는 보충 자료의 그림 23에서 다른 CLIP 모델을 사용하여 이를 더욱 검증합니다. 우리의 훈련 데이터 생성에 사용된 Prompt-to-Prompt를 뛰어넘는 것은 놀라울 수 있지만, 우리는 부록 B에서 이러한 개선의 가능한 원인에 대해 논의합니다.

## 4.2. 소거 연구

그림 10에서는 데이터셋 크기의 선택과 3.1절에서 설명한 데이터셋 필터링 접근 방법에 대한 양적 소거 연구를 제공합니다. 데이터셋의 크기를 줄이는 것은 일반적으로 더 중요한 이미지 편집을 수행하는 능력이 감소하는 결과를 초래하며, 대신에 미묘하거나 스타일리스틱한 이미지 조정만을 수행합니다(그러므로 높은 이미지 유사도 점수를 유지하지만 낮은 방향성 점수를 갖음). 반대로, 우리의 데이터셋 생성에서 CLIP 필터링을 제거하는 것은 입력 이미지와의 전반적인 이미지 일관성을 감소시킵니다.

우리는 또한 그림 4에서 우리의 두 classifier free guidance 규모의 효과에 대한 분석을 제공합니다. $s_T$를 증가시키면 이미지에 적용되는 편집이 강화됩니다(즉, 출력이 지시와 더 일치하게 됩니다), 그리고 $s_I$를 증가시키면 입력 이미지의 공간적 구조를 보존하는 데 도움이 됩니다(즉, 출력이 입력 이미지와 더 일치하게 됩니다). 우리는 $s_T$의 값이 5-10 범위, 그리고 $s_I$의 값이 1-1.5 범위에서 일반적으로 최상의 결과를 생산한다는 것을 발견합니다. 실제로, 그리고 논문에 제시된 결과에 대해, 우리는 일관성과 편집 강도 사이에서 최상의 균형을 얻기 위해 각 예시에 대한 guidance 가중치를 조정하는 것이 유익하다는 것을 발견했습니다.


# 5. 토론

우리는 대규모 언어 모델과 텍스트-이미지 모델 두 가지 대형 사전 훈련된 모델을 결합하여, written 이미지 편집 지시를 따르는 diffusion 모델을 훈련시키기 위한 데이터셋을 생성하는 접근법을 보여줍니다. 우리의 방법은 스타일, 매체 및 기타 맥락적 변경을 포함하여 다양한 설득력 있는 이미지 편집을 생성할 수 있지만, 여전히 몇 가지 한계가 있습니다.

우리 모델은 생성된 데이터셋의 시각적 품질에 의해 제한되며, 따라서 이미지를 생성하는 데 사용된 diffusion 모델(이 경우 Stable Diffusion [51])에 의해 제한됩니다. 또한, 새로운 편집으로 일반화하고 시각적 변화와 텍스트 지시 사이의 올바른 연관성을 만드는 우리 방법의 능력은 GPT-3[7]을 미세 조정하는 데 사용된 인간 작성 지시, GPT-3이 지시를 생성하고 캡션을 수정하는 능력, 그리고 Prompt-to-Prompt[17]가 생성된 이미지를 수정하는 능력에 의해 제한됩니다. 특히, 우리 모델은 객체의 수를 세는 것과 공간적 추론(예: "이미지의 왼쪽으로 옮겨라", "그들의 위치를 바꿔라" 또는 "테이블 위에 두 개의 컵을 놓고 의자에 하나를 놓아라")과 같은 문제에서 어려움을 겪습니다. 또한, 많은 순차적 편집을 수행하는 것이 때때로 누적 아티팩트를 일으키는 원인이 됨을 발견했습니다. 실패 사례는 그림 13에서 찾아볼 수 있습니다. 더욱이, 우리 방법에 기반한 데이터와 사전 훈련된 모델들에 대한 잘 문서화된 편향이 있습니다. 우리 방법에서 편집된 이미지는 이러한 편향을 상속받거나 다른 편향을 도입할 수 있습니다(보충 자료의 그림 14 참조).

위의 한계를 완화하는 것 외에도, 우리의 연구는 공간적 추론을 위한 지시를 어떻게 따를 것인가, 사용자 상호작용과 같은 다른 conditional 모달리티와 지시를 어떻게 결합할 것인가, 지시 기반 편집을 어떻게 평가할 것인가와 같은 질문을 제기합니다. 강화 학습을 사용한 인간의 피드백을 통합하는 것은 향후 연구의 또 다른 중요한 방향이며, 우리 모델과 인간의 의도 사이의 일치를 개선할 수 있습니다.