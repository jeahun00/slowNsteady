
# Abstract
본 연구는 AnyDoor를 소개합니다. 이는 사용자가 지정한 위치에 원하는 형태로 목표 객체를 새로운 장면에 순간 이동시킬 수 있는 diffusion-based 이미지 생성기입니다. 각 객체에 대해 파라미터를 조정하는 대신, 우리의 모델은 단 한 번의 훈련으로 다양한 객체-장면 조합에 일반화할 수 있습니다. 
이와 같은 어려운 zero-shot 환경에서는 특정 객체에 대한 적절한 특성화가 필요합니다. 
이를 위해 일반적으로 사용되는 identity feature를 detail features로 보완합니다. detail features는 외형 세부 사항을 유지하면서도 조명, 방향, 자세 등의 다양한 국부적 변형을 허용하도록 신중하게 설계되어, 객체가 다양한 주변 환경과 유리하게 융합될 수 있도록 지원합니다. 우리는 또한 단일 객체의 다양한 형태(즉, 시간 축을 따라)를 관찰할 수 있는 비디오 데이터셋에서 지식을 차용할 것을 제안합니다. 이는 모델의 일반화 가능성과 견고성을 강화시킵니다. 광범위한 실험을 통해 기존 대안들에 비해 우리의 접근 방식이 우수함을 입증하였으며, 가상 시착, 형태 편집, 객체 교환과 같은 실제 응용에서 큰 잠재력을 가지고 있음을 보여주었습니다. 코드는 github.com/ali-vilab/AnyDoor에서 공개됩니다.

# 1. Introduction
이미지 생성은 diffusion 모델의 급속한 발전과 함께 번성하고 있습니다[22, 37, 40, 41, 43, 62]. 사람들은 텍스트 프롬프트, 낙서, 스켈레톤 맵 또는 기타 조건을 제공하여 선호하는 이미지를 생성할 수 있습니다. 이러한 모델의 힘은 이미지 편집의 가능성도 가져옵니다. 예를 들어, 일부 연구들[5, 24, 63]은 지시를 통해 이미지의 자세, 스타일 또는 내용을 편집하는 방법을 학습합니다. 다른 연구들[53, 59]은 텍스트 프롬프트의 지침을 통해 지역 이미지 영역을 재생성하는 방법을 탐구합니다.

본 논문에서는 "객체 순간이동"을 조사합니다. 이는 목표 객체를 장면 이미지의 원하는 위치에 정확하고 매끄럽게 배치하는 것을 의미합니다. 구체적으로, 우리는 목표 객체를 템플릿으로 사용하여 장면 이미지의 박스/마스크로 표시된 국부 영역을 재생성합니다. 이 능력은 이미지 합성, 효과 이미지 렌더링, 포스터 제작, 가상 시착 등 실용적인 응용에서 중요한 요구사항입니다.

비록 이러한 필요가 강하게 존재하지만, 이 주제는 이전 연구자들에 의해 충분히 탐구되지 않았습니다. Paint-by-Example[56]과 Objectstitch[47]는 특정 장면 이미지 영역을 편집하기 위해 목표 이미지를 템플릿으로 사용하지만, 훈련되지 않은 범주에서는 ID(정체성)-일관성 있는 콘텐츠를 생성할 수 없습니다. 맞춤형 합성 방법[18, 27, 33, 34, 42]은 새로운 개념에 대한 생성을 수행할 수 있지만, 주어진 장면의 특정 위치를 지정할 수는 없습니다. 게다가, 대부분의 맞춤형 방법은 여러 목표 이미지에 대한 미세 조정이 거의 한 시간 동안 필요하므로 실제 응용에서의 실용성을 크게 제한합니다.

이 문제를 해결하기 위해 AnyDoor를 제안합니다. 기존 방법과 달리 AnyDoor는 zero-shot 환경에서 ID-일관성을 유지하면서 고품질의 합성을 생성할 수 있습니다. 이를 위해, 목표 객체를 정체성과 세부사항 관련 특징으로 표현한 다음, 배경 장면과의 상호작용을 통해 합성합니다. 구체적으로, 우리는 ID 추출기를 사용하여 구별 가능한 ID 토큰을 생성하고, 주파수 인지형 세부 추출기를 정교하게 설계하여 세부 맵을 보완 자료로 얻습니다. 그런 다음, 우리는 ID 토큰과 세부 맵을 사전 학습된 text-to-image diffusion 모델에 주입하여 원하는 합성을 생성하는 지침으로 사용합니다. 생성된 콘텐츠를 더욱 사용자 정의할 수 있도록, 우리는 추가적인 제어(예: 사용자가 그린 마스크)를 활용하여 객체의 형태/자세를 지정하는 방법을 탐구합니다. 높은 다양성을 가진 맞춤형 객체 생성을 학습하기 위해, 우리는 동일한 객체의 외형 변화를 학습하기 위해 비디오에서 이미지 쌍을 수집하고, 시나리오 다양성을 보장하기 위해 대규모 통계 이미지를 활용합니다.

이러한 기술들을 갖춘 AnyDoor는 zero-shot 맞춤화에서 뛰어난 능력을 보여줍니다. 그림 1에서, AnyDoor는 형태 제어와 함께 새로운 개념의 합성에서 유망한 성능을 보여줍니다(상단 행). 또한, AnyDoor는 장면 이미지의 특정 지역을 편집하는 높은 제어 가능성을 가지고 있기 때문에, 여러 객체를 합성하는 것(중간 행)에도 쉽게 확장될 수 있으며, 이는 많은 맞춤형 생성 방법들이 탐구하는 뜨겁고 도전적인 주제입니다[3, 19, 27, 34]. 더 나아가, AnyDoor의 높은 생성 충실도와 품질은 객체 이동 및 교체와 같은 더욱 환상적인 응용 가능성을 열어줍니다(하단 행). 우리는 AnyDoor가 이미지 입력을 활용한 다양한 이미지 생성 및 편집 작업의 기초 솔루션으로 작용하고, 더 많은 멋진 응용 프로그램에 활력을 불어넣는 기본 능력으로 작용하기를 바랍니다.

# 2. Related Work

### local image editing. 
대부분의 이전 연구들은 텍스트 지침을 통해 지역 이미지 영역을 편집하는 데 중점을 두고 있습니다. Blended Diffusion[2]은 마스크된 영역에서 다단계 블렌딩을 수행하여 더 조화로운 출력을 생성합니다. Inpaint Anything[59]은 SAM[26]과 Stable Diffusion[41]을 사용하여 소스 이미지의 객체를 텍스트로 설명된 목표 객체로 교체합니다. Paint-by-Example[56]은 CLIP[39] 이미지 인코더를 사용하여 목표 이미지를 임베딩으로 변환하여 장면 이미지에 의미론적으로 일관된 객체를 그립니다. ObjectStitch[47]은 [56]과 유사한 솔루션을 제안하며, CLIP 이미지 인코더의 출력을 텍스트 인코더에 맞추기 위해 콘텐츠 어댑터를 훈련하여 diffusion 과정을 안내합니다. 그러나 이러한 방법들은 생성에 대해 대략적인 지침만 제공할 수 있으며, 훈련되지 않은 새로운 개념에 대해 ID-일관성 있는 결과를 합성하는 데 자주 실패합니다.

### Customized image generation. 
맞춤형 또는 주제 중심 생성은 몇 개의 목표 이미지와 관련된 텍스트 프롬프트를 제공받아 특정 객체에 대한 이미지를 생성하는 것을 목표로 합니다. 일부 연구[9, 18, 42]는 목표 개념을 설명하기 위해 "어휘"를 미세 조정합니다. Cones[33]는 참조된 객체에 해당하는 뉴런을 찾습니다. 비록 이 방법들이 높은 충실도의 이미지를 생성할 수 있지만, 사용자는 시나리오와 목표 객체의 위치를 지정할 수 없습니다. 게다가, 시간 소모적인 미세 조정은 이 방법들이 대규모 응용에서 사용되는 것을 방해합니다. 최근에 BLIP-Diffusion[28]은 BLIP-2[29]를 활용하여 이미지와 텍스트를 정렬하여 zero-shot 맞춤화를 수행합니다. Fastcomposer[52]는 특정 텍스트 임베딩과 이미지 표현을 결합하여 다중 인물 생성을 수행합니다. 몇몇 동시 연구들[30, 58, 61]도 하나의 참조 이미지를 사용하여 생성 결과를 맞춤화하는 방법을 탐구하지만, 세부 사항을 유지하는 데 실패합니다.

### Image harmonization.
전통적인 이미지 합성 파이프라인은 전경 객체를 잘라서 주어진 배경에 붙이는 것입니다. 이미지 조화(image harmonization)[7, 14, 20, 48]는 붙여진 영역의 조명과 색상을 더 합리적으로 조정할 수 있습니다. DCCF[55]는 전경을 더 잘 조화시키기 위해 피라미드 필터를 설계합니다. CDTNet[15]은 이중 트랜스포머를 활용합니다. HDNet[8]은 전역 및 지역 일관성을 모두 고려하는 계층 구조를 제안하여 최첨단 성과를 달성합니다. 그럼에도 불구하고, 이러한 방법들은 구조, 뷰, 전경 객체의 자세 편집 또는 그림자와 반사 생성과 같은 저수준 변화만 탐구합니다.

# 3. Method
AnyDoor의 파이프라인은 그림 2에 설명되어 있습니다. 목표 객체, 장면 및 위치가 주어지면 AnyDoor는 높은 충실도와 다양성으로 객체-장면 합성을 생성합니다. 핵심 아이디어는 객체를 정체성과 세부 사항 관련 특징으로 표현하고, 사전 학습된 diffusion 모델에 이러한 특징을 주입하여 주어진 장면에 재구성하는 것입니다. 외형 변화를 학습하기 위해, 우리는 대규모 데이터를 활용하여 비디오와 이미지를 모두 훈련에 사용합니다.

### 3.1. Identity Feature Extraction

우리는 사전 학습된 시각 인코더를 활용하여 목표 객체의 정체성 정보를 추출합니다. 이전 연구들[47, 56]은 CLIP[39] 이미지 인코더를 선택하여 목표 객체를 임베드합니다. 그러나 CLIP은 텍스트-이미지 쌍으로 훈련되어 대략적인 설명만을 포함하기 때문에, 의미 수준의 정보는 임베드할 수 있지만 객체의 정체성을 유지하는 변별적인 표현을 제공하는 데 어려움을 겪습니다. 이 문제를 극복하기 위해, 우리는 다음과 같은 업데이트를 수행합니다.

#### Background removal. 
목표 이미지를 ID 추출기에 입력하기 전에, 우리는 분할기를 사용하여 배경을 제거하고 객체를 이미지 중앙에 정렬합니다. 분할기 모델은 자동[26, 38] 또는 상호작용[11, 12, 32] 방식일 수 있습니다. 이 작업은 더 깔끔하고 변별적인 특징을 추출하는 데 유용한 것으로 입증되었습니다.

#### Self-supervised representation. 
본 연구에서는 자가 지도 학습 모델이 더 변별적인 특징을 유지하는 강력한 능력을 보여준다는 것을 발견했습니다. 대규모 데이터셋에서 사전 학습된 자가 지도 학습 모델은 인스턴스 검색 능력을 자연스럽게 갖추고 있어, 객체를 증강 불변 특징 공간으로 투영할 수 있습니다. 우리는 현재 가장 강력한 자가 지도 학습 모델인 DinoV2[36]를 ID 추출기의 백본으로 선택하여, 이미지를 전역 토큰 $\mathbf{T}_g^{1 \times 1536}$ 및 패치 토큰 $\mathbf{T}_p^{256 \times 1536}$으로 인코딩합니다. 우리는 두 종류의 토큰을 결합하여 더 많은 정보를 보존합니다. 단일 선형 레이어를 사용하여 프로젝터가 이러한 토큰을 사전 학습된 text-to-image UNet의 임베딩 공간과 정렬할 수 있음을 발견했습니다. 투영된 토큰 $\mathbf{T}_{\text{ID}}^{257 \times 1024}$는 우리의 ID 토큰으로 기록됩니다.

### 3.2. Detail Feature Extraction

ID 토큰이 저해상도(16 × 16)로 표현된다는 점을 고려하면, 저수준의 세부 사항을 적절하게 유지하기 어려울 것입니다. 따라서 보완적으로 세부 사항 생성을 위한 추가 지침이 필요합니다.

#### Collage representation. 
[6, 44]에서 영감을 받아, 콜라주를 제어 수단으로 사용하면 강력한 사전 정보를 제공할 수 있습니다. 우리는 "배경 제거 객체"를 장면 이미지의 주어진 위치에 꿰매어 넣는 방법을 시도합니다. 이 콜라주를 통해 생성 충실도가 크게 향상되었지만, 생성된 결과가 주어진 목표와 너무 유사하여 다양성이 부족합니다. 이 문제를 해결하기 위해, 우리는 콜라주가 너무 많은 외형 제약을 주지 않도록 정보 병목을 설정하는 방법을 탐구합니다. 구체적으로, 우리는 객체를 표현하기 위해 고주파 맵을 설계하였으며, 이는 자세, 조명, 방향 등과 같은 다양한 국부 변형을 허용하면서 세부 사항을 유지할 수 있습니다.

#### High-frequency map. 
우리는 목표 객체의 고주파 맵을 다음과 같이 추출합니다.

$$I_h = \left( I_{\text{gray}} \otimes K_h + I_{\text{gray}} \otimes K_v \right) \odot I \odot M_{\text{erode}}$$

여기서 $K_h$, $K_v$는 수평 및 수직 Sobel[23] 커널을 나타내며, 고역통과 필터 역할을 합니다. $\otimes$, $\odot$는 각각 컨볼루션 및 하다마드 곱을 의미합니다. 주어진 이미지 $I$에서, 우리는 먼저 이러한 고역통과 필터를 사용하여 고주파 영역을 추출한 다음, 하다마드 곱을 사용하여 RGB 색상을 추출합니다. 우리는 또한 목표 객체의 외곽 근처의 정보를 필터링하기 위해 침식 마스크 $M_{\text{erode}}$를 추가합니다.

그림 3에서 시각화된 것처럼, DINOV2에 의해 생성된 토큰은 전체 구조에 더 중점을 두어 첫 번째 행의 배낭 로고와 같은 세부 사항을 인코딩하기 어렵게 만듭니다. 반면에, 고주파 맵은 보완적으로 이러한 세부 사항을 처리하는 데 도움이 될 수 있습니다.

#### Shape control
우리는 객체의 제스처를 나타내기 위해 형태 마스크를 사용합니다. 사용자 입력을 시뮬레이션하기 위해, 우리는 다양한 비율로 실제 마스크를 다운샘플링하고 세부 사항을 제거하기 위해 랜덤 확장/침식을 적용합니다. 단일 박스 입력을 처리할 수 있는 능력을 유지하기 위해, 우리는 내부 박스 영역을 마스크로 사용하는 확률을 0.3으로 설정합니다. 훈련 중에 객체 경계는 형태 마스크와 정렬됩니다. 따라서 사용자는 추론 중에 대략적인 형태 마스크를 그려서 목표 객체의 형태를 제어할 수 있습니다.

콜라주와 윤곽 맵을 얻은 후, 우리는 그것들을 연결하고 디테일 추출기에 입력합니다. 디테일 추출기는 ControlNet 스타일의[62] UNet 인코더로, 계층적 해상도의 일련의 디테일 맵을 생성합니다.

### 3.3. Feature Injection

ID 토큰과 디테일 맵을 얻은 후, 우리는 이를 사전 학습된 text-to-image diffusion 모델에 주입하여 생성을 안내합니다. 우리는 이미지를 잠재 공간으로 투사하고 확률적 샘플링을 수행하는 Stable Diffusion[41]을 선택합니다. 우리는 사전 학습된 UNet을 $\tilde{\mathbf{x}}_\theta$로 표기하며, 이는 초기 잠재 노이즈 $\epsilon \sim \mathcal{U}([0, 1])$에서 디노이징을 시작하고, 조건으로 텍스트 임베딩 $c$를 사용하여 새로운 이미지 잠재 벡터 $\mathbf{z}_t$를 생성합니다:

$$
\mathbf{z}_t = \alpha_t \tilde{\mathbf{x}}_\theta (\epsilon, c) + \sigma_t \epsilon.
$$

훈련 감독은 다음과 같은 평균 제곱 오차 손실을 사용합니다:

$$
\mathbb{E}_{\mathbf{x}, c, \epsilon, t} \left( \left\| \tilde{\mathbf{x}}_\theta (\alpha_t \mathbf{x} + \sigma_t \epsilon, c) - \mathbf{x} \right\|_2^2 \right).
$$

여기서 $\mathbf{x}$는 실제 이미지 잠재 벡터이며, $t$는 diffusion 시간 단계, $\alpha_t$와 $\sigma_t$는 디노이징 하이퍼파라미터입니다.

이 작업에서, 우리는 텍스트 임베딩 $c$를 우리의 ID 토큰으로 대체하여, 이를 교차 주의를 통해 각 UNet 레이어에 주입합니다. 디테일 맵의 경우, 각 해상도에서 UNet 디코더 특징과 결합합니다. 훈련 중에, 우리는 사전 학습된 UNet 인코더의 파라미터를 고정하여 사전 정보를 보존하고 UNet 디코더를 새로운 작업에 맞게 조정합니다.

### 3.4. Training Strategies

#### Image pair collection. 
이상적인 훈련 샘플은 "다른 장면에서 동일한 객체"의 이미지 쌍이지만, 이는 기존 데이터셋에서 직접 제공되지 않습니다. 대안으로, 이전 연구들[47, 56]은 단일 이미지를 활용하고 회전, 뒤집기, 탄성 변형과 같은 증강을 적용합니다. 그러나 이러한 단순한 증강은 자세와 뷰의 현실적인 변형을 잘 나타내지 못합니다.

이 문제를 해결하기 위해, 본 연구에서는 동일한 객체를 포함하는 다양한 프레임을 캡처하기 위해 비디오 데이터셋을 활용합니다. 데이터 준비 파이프라인은 그림 4에 설명되어 있으며, 비디오 분할/추적 데이터를 예제로 활용합니다. 비디오의 경우, 우리는 두 프레임을 선택하고 전경 객체에 대한 마스크를 취합니다. 그런 다음, 한 이미지의 배경을 제거하고 마스크 주위로 잘라 목표 객체로 사용합니다. 이 마스크는 변형 후 마스크 제어로 사용할 수 있습니다. 다른 프레임의 경우, 우리는 박스를 생성하고 박스 영역을 제거하여 장면 이미지를 얻으며, 마스크가 없는 이미지는 훈련의 실제 데이터로 사용할 수 있습니다. 사용된 전체 데이터는 표 1에 나와 있으며, 자연 장면, 가상 시착, 주목도, 다중 뷰 객체와 같은 다양한 도메인을 포함합니다.
#### Adaptive timestep sampling
비디오 데이터는 외형 변화를 학습하는 데 유용할 수 있지만, 프레임 품질은 보통 저해상도나 모션 블러로 인해 만족스럽지 않습니다. 반면, 이미지는 고품질 세부 사항과 다양한 시나리오를 제공할 수 있지만 외형 변화는 부족합니다. 비디오 데이터와 이미지 데이터를 모두 활용하기 위해, 우리는 적응형 시간 단계 샘플링(adaptive timestep sampling)을 개발하여 다양한 데이터 모달리티가 디노이징 훈련의 각 단계에서 혜택을 받을 수 있도록 합니다.

기존 diffusion 모델[41]은 각 훈련 데이터에 대해 시간 단계(T)를 균등하게 샘플링합니다. 그러나 초기 디노이징 단계는 주로 전체 구조, 자세, 뷰를 생성하는 데 중점을 두고, 후반 단계는 텍스처와 색상과 같은 세부 사항을 다룹니다. 따라서 비디오 데이터의 경우, 우리는 초기 디노이징 단계(500-1000)를 샘플링하는 확률을 50% 증가시켜 외형 변화를 더 잘 학습할 수 있도록 합니다. 이미지의 경우, 우리는 후반 단계(0-500)의 확률을 50% 증가시켜 세부 사항을 다루는 방법을 학습하도록 합니다.

