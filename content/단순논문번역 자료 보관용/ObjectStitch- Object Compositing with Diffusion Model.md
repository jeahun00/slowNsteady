# Abstract

2D 이미지를 기반으로 한 객체 합성은 현실적인 결과를 생성하기 위해 색상 조화, 기하학적 보정, 그림자 생성 등 여러 처리 단계를 포함하기 때문에 도전적인 문제입니다. 게다가 합성을 위한 학습 데이터 쌍을 주석으로 다는 것은 전문가의 상당한 수작업이 필요하며, 확장성이 거의 없습니다. 따라서 최근의 생성 모델 발전을 바탕으로, 본 연구에서는 conditional diffusion models의 힘을 활용하여 객체 합성을 위한 self-supervised framework를 제안합니다. 우리의 프레임워크는 통합 모델에서 객체 합성 작업을 포괄적으로 해결할 수 있으며, 생성된 객체의 관점, 기하학, 색상 및 그림자를 변환하면서 수작업 레이블링이 필요하지 않습니다. 입력 객체의 특성을 보존하기 위해, 범주적 의미와 객체의 외관을 유지하는 데 도움을 주는 content adaptor를 도입합니다. 또한, 생성기의 충실도를 향상시키기 위해 데이터 증강 방법을 채택합니다. 우리의 방법은 다양한 실제 이미지에 대한 사용자 연구에서 합성된 결과 이미지의 현실성 및 신뢰성 측면에서 관련 기준을 능가합니다.

# Introduction

이미지 합성은 주어진 이미지의 객체를 다른 이미지에 현실감 있게 삽입하는 것을 목표로 하는 이미지 편집의 필수 작업입니다. 전통적으로, 객체를 새로운 장면에 합성하는 데는 색상 조화 [6, 7, 19, 51], 재조명 [52], 그림자 생성 [16, 29, 43]을 포함하여 많은 하위 작업이 필요하며, 이는 객체가 새로운 이미지에 자연스럽게 어우러지도록 합니다. Tab. 1에서 보듯이, 대부분의 이전 방법들 [6, 7, 16, 19, 28, 43]은 이미지 합성에 필요한 단일 하위 작업에 집중합니다. 따라서, 입력 객체가 배경 장면과 일치하는 색상, 조명, 그림자를 갖도록 합성 이미지를 얻으려면 이들 작업을 적절하게 결합해야 합니다. Fig. 1에서 보듯이, 이러한 방식으로 생성된 결과물은 삽입된 객체의 관점이 전체 배경과 다르기 때문에 여전히 부자연스럽게 보입니다.

2D 이미지 합성에서는 객체와 배경 장면의 기하학을 정확히 이해해야 하는 기하학 조화 및 새로운 뷰 합성이 종종 간과되었습니다. 이전 연구들 [15, 21, 22]은 조명 위치와 깊이와 같은 명시적 배경 정보를 이용한 3D 객체 합성을 다루었습니다. 최근에는 [28]이 GANs를 이용하여 객체의 호모그래피를 추정하는 방법을 제안했지만, 이 방법은 실내 장면에 가구를 배치하는 것에만 제한됩니다. 본 논문에서는 diffusion-based generative model을 사용하여 입력 객체의 기하학을 배경 이미지와 함께 색상, 조명 및 그림자와 조화시킬 수 있는 일반적인 이미지 합성 방법을 제안합니다.

최근 몇 년간, GANs [4, 10, 17, 20] 및 diffusion models [1, 14, 30, 32, 33, 38, 39]와 같은 생성 모델들은 현실적인 이미지를 합성하는 데 큰 잠재력을 보여주었습니다. 특히 diffusion model 기반 프레임워크는 다재다능하며 이미지 편집 [1, 23, 32] 및 기타 응용 분야 [11, 31, 35]에서 다양한 이전 방법들을 능가합니다. 그러나 대부분의 이미지 편집 diffusion models는 텍스트 입력을 사용하여 이미지를 조작하는 데 초점을 맞추고 있으며 [1, 3, 9, 23, 33], 이는 이미지 합성에는 충분하지 않습니다. 왜냐하면 언어적 표현은 주어진 객체 이미지의 세부 사항을 완전히 캡처하거나 정체성과 외관을 보존할 수 없기 때문입니다. 최근 [23, 39]와 같은 연구들은 객체의 주요 특징을 보존하면서 다양한 컨텍스트를 생성하는 데 초점을 맞추고 있지만, 이러한 모델들은 객체 합성과는 다른 작업을 위해 설계되었습니다. 게다가 [23]은 각 입력 객체에 대해 모델을 미세 조정해야 하며 [39] 역시 동일한 객체의 여러 이미지에서 미세 조정을 필요로 합니다. 따라서 이들은 일반적인 객체 합성에는 한계가 있습니다.

본 연구에서는 diffusion models를 활용하여 색상 조화, 재조명, 기하학적 보정 및 그림자 생성을 포함한 이미지 합성의 여러 측면을 동시에 처리합니다. 텍스트 가이드 대신 이미지 가이드를 사용하여 생성된 합성 이미지에서 원래 객체의 정체성과 외관을 보존하고자 합니다. 구체적으로, 우리의 모델은 (i) 소스 객체, (ii) 대상 배경 이미지, (iii) 객체를 삽입할 위치를 지정하는 경계 상자를 주어 합성 이미지를 생성합니다. 제안된 프레임워크는 content adaptor와 generator 모듈로 구성됩니다: content adaptor는 색상 및 형태와 같은 저수준 세부 사항과 고수준 의미를 모두 포함하는 입력 객체로부터 표현을 추출하도록 설계되었습니다. generator 모듈은 배경 장면을 보존하면서 생성 품질과 다재다능성을 향상시킵니다. 우리의 프레임워크는 완전히 self-supervised 방식으로 훈련되며, 훈련 과정에서 특정 작업에 대한 레이블링이 필요하지 않습니다. 게다가 다양한 데이터 증강 기법을 적용하여 출력의 충실도와 현실성을 더욱 향상시킵니다. 우리는 이미지 합성을 위한 실제 사용 사례를 가깝게 시뮬레이션한 실제 데이터셋에서 제안된 방법을 평가합니다.

우리의 기여는 다음과 같이 요약됩니다:
- 관점, 기하학, 조명 및 그림자와 같은 합성의 여러 측면을 처리할 수 있는 첫 번째 diffusion model 기반 객체 합성 프레임워크를 제시합니다.
- 이미지 가이드를 가능하게 하는 다중 모드 임베딩을 학습하는 content adaptor 모듈을 제안합니다.
- 우리의 프레임워크는 특정 작업에 대한 주석 없이 self-supervised 방식으로 훈련되며, 생성의 충실도를 향상시키기 위해 데이터 증강 기법을 사용합니다.
- 다양한 이미지와 수동으로 주석이 달린 객체 크기와 위치를 포함하는 고해상도 실제 데이터셋을 수집합니다.

# 2. 관련 연구

## 2.1 이미지 합성
이미지 합성은 주어진 전경 이미지의 객체를 배경 이미지에 삽입하는 reference-guided image editing에서 도전적인 작업입니다. 생성된 합성 이미지는 원래 객체의 외관이 보존된 상태에서 현실적으로 보이는 것이 기대됩니다. 이전 연구들은 이 문제의 개별 측면에 초점을 맞추는 경우가 많았습니다. 예를 들어, 기하학적 보정 [2, 28], 이미지 조화 [6, 7, 51], 이미지 매팅 [50], 그림자 생성 [29] 등이 있습니다.

Lin 등 [28]은 객체와 배경 이미지 간의 기하학적 불일치를 수정하기 위해 반복적인 GAN 기반 프레임워크를 제안합니다. 그들의 모델에서는 STNs [18]을 생성기 네트워크에 통합하여 일련의 왜곡 업데이트를 예측합니다. 이들의 연구는 합성 이미지에서 장면 기하학의 현실성을 향상시킬 수 있지만, 실내 장면에 가구를 삽입하는 데만 제한됩니다. Azadi 등 [2]은 이진 합성에 초점을 맞추어, 두 객체 간의 상호 작용을 캡처하는 합성-분해 네트워크를 사용합니다.

이미지 조화는 입력 객체와 배경 이미지 간의 불일치를 최소화하는 것을 목표로 합니다. 전통적인 방법 [24, 46]은 주로 색상 통계를 얻은 다음 이 정보를 전경과 배경 간에 전송하는 데 중점을 둡니다. 최근 연구들은 딥 뉴럴 네트워크를 통해 이 문제를 해결하려고 합니다. [6, 7]은 이미지 조화를 domain adaptation 문제로 재구성하며, [19]는 이를 표현 융합 문제로 변환하고 self-supervised training을 활용합니다.

그림자 합성은 현실적인 합성 이미지를 생성하는 데 필수적이지만, 이전의 이미지 합성 방법에서는 종종 간과되었습니다. SGRNet [16]은 이 작업을 그림자 마스크 생성 단계와 그림자 채우기 단계로 나눕니다. SSN [43]은 소프트 쉐도우에 초점을 맞추어, 그림자 생성을 위한 단서로서 ambient occlusion map을 예측합니다.

여러 이미지 합성 하위 문제를 동시에 다루는 연구들도 있습니다. Chen 등 [5]은 기하학적, 색상 및 경계 일관성을 갖춘 그럴듯한 이미지를 생성하기 위해 여러 네트워크 모듈을 포함하는 시스템을 개발했습니다. 그러나 생성된 객체의 포즈는 마스크 입력에 의해 제한되며, 모델은 동물과 같은 비강체 객체에 일반화할 수 없습니다. 우리가 제안한 모델은 또한 이미지 합성 문제를 통합된 방식으로 처리하여, 배경과 조화롭고 기하학적으로 일관된 전경 객체를 생성하면서 새로운 뷰와 그림자를 합성합니다.

## 2.2 Guided Image Synthesis

최근 몇 년 동안 diffusion models에서 큰 발전이 있었습니다. Diffusion models는 몇 가지 주요 연구 [14, 44, 45]를 기반으로 한 딥 생성 모델 군으로, 두 개의 Markov chains로 정의됩니다. 전방 과정에서는 데이터에 점진적으로 노이즈를 추가하고, 역방 과정에서는 노이즈에서 데이터를 복원하는 법을 학습합니다. 이 분야의 연구가 성장함에 따라 diffusion models는 text-to-image generation [9, 33, 38], image editing [1, 23, 32], guided image synthesis [30] 등 다양한 응용 분야에서 그 잠재력을 보여주었습니다.

Text-to-image synthesis는 지난 몇 년 동안 인기 있는 연구 주제였습니다. Stable Diffusion [38]은 이 작업에 중요한 기여를 했으며, latent space에서 diffusion model을 적용하여 계산 비용을 줄였습니다. 다른 연구들은 더 유연하고 제어 가능한 text-driven image editing 방법을 탐구합니다. Avrahami 등 [1]은 Blended Diffusion 프레임워크에서 입력의 노이즈 버전과 로컬 텍스트 가이드된 diffusion latent를 융합하여 부드러운 전환을 생성하는 알고리즘을 설계했습니다. 마찬가지로, GLIDE [33]도 이 특정 작업에 대해 미세 조정한 후 텍스트 기반 inpainting을 처리할 수 있습니다. 일부 방법들은 사용자에게 더 직접적인 이미지 제어를 제공합니다. SDEdit [32]는 이미지에 스트로크 페인팅을 허용하여, 입력에 노이즈를 추가하고 확률적 미분 방정식을 통해 디노이징하여 사용자 입력을 이미지에 혼합합니다. SDG [30]는 생성의 각 반복에서 Semantic Diffusion Guidance를 주입하여 다중 모드 지침을 제공할 수 있습니다.

Diffusion models의 다재다능함과 생성 품질은 여러 번 입증되었지만, 객체의 외관을 보존하는 것은 여전히 매우 도전적인 문제입니다. Ruiz 등 [39]은 재구성 손실과 클래스별 사전 보존 손실을 사용하여 모델을 최적화함으로써 이 문제를 해결합니다. 이들의 접근 방식은 객체의 세부 사항을 보존할 수 있지만, 동일한 객체의 여러 이미지를 사용하여 해당 객체에 모델을 미세 조정해야 합니다. Kawar 등 [23]은 초기 텍스트 임베딩과 최적화된 임베딩을 보간하여 재구성을 처리하지만, 이 방법은 주로 객체에 편집을 적용하는 데 설계되어 있으며, 다른 컨텍스트에서 동일한 객체를 생성하는 것은 아닙니다. 본 논문에서는 diffusion models에 기반한 최초의 생성 객체 합성 프레임워크를 제안하며, 조화롭고 기하학적으로 올바른 주제를 새로운 뷰와 그림자로 생성합니다. 더 나아가, 합성된 이미지에서 원래 객체의 특성을 보존합니다.

# 3. 제안 방법

생성 객체 합성 문제를 다음과 같이 정의합니다: 객체 이미지 $I_o \in \mathbb{R}^{H_o \times W_o \times 3}$, 배경 이미지 $I_{bg} \in \mathbb{R}^{H_{bg} \times W_{bg} \times 3}$, 및 해당 이진 마스크 $M \in \mathbb{R}^{H_{bg} \times W_{bg} \times 1}$로 구성된 입력 삼중항 $(I_o, I_{bg}, M)$이 주어지며, 원하는 객체 위치가 0으로 설정되고 나머지는 1로 설정됩니다. 목표는 마스크 처리된 영역 $I_{bg} \otimes M$에 입력 객체를 합성하는 것입니다. $M$은 합성된 객체의 위치와 크기에 대한 소프트 제약으로 간주됩니다. 생성된 출력 이미지는 현실적으로 보여야 하며, 객체의 외관이 보존되어야 합니다. 우리의 문제 설정은 텍스트 가이드 이미지 생성 및 인페인팅과 달리, 조건 입력이 텍스트 프롬프트가 아닌 참조 객체 이미지입니다. 텍스트-이미지 diffusion models [37, 38, 40]의 성공에 영감을 받아, 이러한 사전 학습된 모델을 활용하여 생성 합성을 수행하는 방법을 설계했습니다.

우리의 프레임워크 개요는 Fig. 2에 나와 있습니다. 이는 객체 이미지 인코더가 객체로부터 의미론적 특징을 추출하고, 조건부 diffusion generator를 포함합니다. 사전 학습된 텍스트-이미지 diffusion models의 힘을 활용하기 위해, 객체 인코더와 조건부 생성기 사이의 격차를 줄일 수 있는 content adaptor를 도입했습니다. 이는 시각적 토큰 시퀀스를 텍스트 토큰 시퀀스로 변환하여 이미지와 텍스트 간의 도메인 격차를 극복합니다. 이 디자인은 객체의 외관을 보존하는 데 도움을 줍니다. 우리는 두 단계의 학습 과정을 제안합니다: 첫 번째 단계에서는 content adaptor가 객체의 고수준 의미론을 유지하기 위해 대규모 이미지/텍스트 쌍으로 훈련됩니다. 두 번째 단계에서는 diffusion generator의 맥락에서 객체의 주요 정체성 특징을 인코딩하도록 훈련되어, 원래 이미지에서 객체의 시각적 재구성을 장려합니다. 마지막으로, 생성기 모듈은 cross attention blocks를 통해 adaptor가 생성한 임베딩에 미세 조정됩니다. 모든 단계는 객체 합성 학습 데이터를 얻기 위해 명시적 주석을 피하기 위해 self-supervised 방식으로 훈련됩니다.

## 3.1 생성기

Fig. 2에 나타난 바와 같이, 우리는 사전 학습된 텍스트-이미지 diffusion model 아키텍처를 활용하여 합성 작업에 맞게 수정합니다. 이를 위해 (i) 입력에 마스크를 도입하고, (ii) U-Net 입력을 조정하여 홀 외부의 원래 배경 이미지와 홀 내부의 노이즈를 마스크 블렌딩과 함께 포함시킵니다. 모델을 가이던스 임베딩 $E$에 조건화하기 위해, 다음과 같이 attention 메커니즘이 적용됩니다:


$$
\text{Softmax} \left( \frac{(W_Q E_x)(W_K E)^T}{\sqrt{d}} \right) W_V E = AV \tag{1}
$$

여기서 $E_x$는 디노이징 오토인코더의 중간 표현이고, $W_Q \in \mathbb{R}^{d \times d_x}$, $W_K \in \mathbb{R}^{d \times d_e}$ 및 $W_V \in \mathbb{R}^{d \times d_e}$는 임베딩 매트릭스입니다. 마스크 영역 외부의 배경은 생성 객체 합성을 위해 완벽하게 보존되어야 합니다. 따라서 입력 마스크 $M$을 사용하여 배경 이미지 $I_{bg}$와 생성된 이미지 $I_{out}$을 블렌딩합니다. 결과적으로, 생성기는 마스크 처리된 영역 $I_{out} \otimes M$만 디노이즈합니다. 우리의 작업에 이 모델을 사용하기 위한 간단한 방법(기준선)은 객체 이미지에 대한 이미지 캡션을 적용하고, 결과 캡션을 조건으로 diffusion model에 직접 입력하는 것입니다. 그러나 텍스트 임베딩은 세밀한 시각적 세부 사항을 캡처할 수 없습니다. 따라서 주어진 객체 이미지를 직접 활용하기 위해, content adaptor를 도입하여 사전 학습된 시각 인코더의 시각적 특징을 텍스트 특징(토큰)으로 변환하여 생성기의 조건으로 사용합니다.

## 3.2 Content Adaptor

핵심 정체성 정보를 잃지 않기 위해, 텍스트 인코더 대신 이미지 인코더를 사용하여 입력 객체 이미지로부터 임베딩을 생성합니다. 그러나 이미지 임베딩은 두 가지 이유로 diffusion model에 의해 효과적으로 활용될 수 없습니다:

- 이미지 임베딩 $\tilde{E}$와 텍스트 임베딩 $E$는 다른 도메인에서 유래합니다. diffusion model이 $E$로 훈련되었기 때문에 이미지 임베딩 시퀀스로부터 의미 있는 내용을 생성할 수 없습니다.
- $\tilde{E} \in \mathbb{R}^{k \times 257 \times 1024}$와 $E \in \mathbb{R}^{k \times 77 \times 768}$의 차원 불일치, 여기서 $k$는 배치 크기입니다.

따라서 위의 관찰을 바탕으로, 우리는 Fig. 3에 나타난 시퀀스-투-시퀀스 번역기 아키텍처를 개발했습니다. 이미지-캡션 쌍을 입력 튜플 $(I, t)$로 주어지면, 두 개의 사전 학습된 ViT-L/14 인코더 $C_t$, $C_i$를 CLIP [36]에서 사용하여 각각 텍스트 임베딩 $E = C_t(t)$와 이미지 임베딩 $\tilde{E} = C_i(I)$를 생성합니다. Adaptor $T$는 세 가지 구성 요소로 구성됩니다: 1D 합성곱 층, attention blocks [48], MLP. 여기서 1D 합성곱은 임베딩의 길이를 257에서 77로 수정하고, MLP는 임베딩 차원을 1024에서 768로 매핑하며, attention blocks는 텍스트 도메인과 이미지 도메인 간의 격차를 줄입니다. 이 모듈을 훈련하기 위해 두 단계의 최적화 방법을 설계했으며, 이는 섹션 3.3.2에 설명되어 있습니다.

## 3.3 Self-supervised Framework

주석이 달린 이미지 합성 훈련 데이터셋은 공개적으로 이용할 수 없으며, diffusion model을 훈련하기에 충분하지 않습니다. 또한, 이러한 데이터를 수작업으로 주석 달기에는 극도로 어려움이 있습니다. 따라서 우리는 self-supervised 훈련 방식과 실제 시나리오를 시뮬레이션하는 합성 데이터 생성 접근 방식을 제안합니다. 또한, 훈련 데이터를 풍부하게 하고 모델의 견고성을 향상시키기 위해 데이터 증강 방법을 도입합니다.

### 3.3.1 데이터 생성 및 증강

#### Training data generation
우리는 Pixabay에서 합성 훈련 데이터를 수집하고 객체 인스턴스 세그멘테이션 모델 [26]을 사용하여 panoptic segmentation 및 분류 레이블을 예측합니다. 먼저 매우 작거나 큰 크기의 객체를 제거하여 데이터셋을 필터링합니다. 그런 다음, 입력 이미지와 배경 이미지가 서로 다른 장면 기하학과 조명 조건을 가지는 실제 사용 사례 시나리오를 시뮬레이션하기 위해 공간적 및 색상 변형을 적용합니다.

Fig. 4의 상단 행은 이 과정을 보여줍니다. [8]에서 영감을 받아 객체 경계 상자의 네 점을 무작위로 변형하여 투사 변환을 적용한 다음, \([-θ, θ]\) 범위 내에서 무작위 회전 (\(θ = 20°\))과 색상 변형을 적용합니다. 세그멘테이션 마스크(이미지와 동일하게 변형됨)는 객체를 추출하는 데 사용됩니다.

이 데이터 합성 파이프라인은 완전히 제어 가능하며 훈련 중 데이터 증강으로도 사용할 수 있습니다. 또 다른 주요 이점은 수작업 레이블링이 필요 없다는 것입니다. 원래 이미지를 정답으로 사용하기 때문입니다. 우리는 객체를 포함하는 경계 상자를 마스크로 사용하여 그림자 생성 공간을 제공합니다. 우리는 이 방식이 모델이 공간 변환을 적용하고, 새로운 뷰를 합성하며 그림자와 반사를 생성할 수 있을 만큼 유연하다는 것을 발견했습니다.

#### Real-world evaluation data
특정 작업에 대한 기존 데이터셋이 없기 때문에, 우리는 실제 사용 사례를 가깝게 시뮬레이션한 새로운 데이터셋을 수동으로 수집하여 객체 합성을 위한 평가 벤치마크로 사용합니다. 이 데이터셋은 일반적인 객체(차량 및 야생 동물과 같은 비강체 객체 포함)의 503 쌍과 다양한 배경 이미지(실내 및 실외 장면 포함)로 구성됩니다. 또한 전경과 배경 간의 조명 조건 또는 관점 불일치가 큰 도전적인 사례도 포함됩니다. 데이터셋 이미지는 Pixabay에서 수집되었습니다. 레이블링 절차는 입력 객체가 배경 이미지의 목표 위치에 배치되고 사용자가 원하는 크기로 확장되는 실제 시나리오를 시뮬레이션합니다. 합성 영역은 객체 주위의 느슨한 경계 상자로 결정됩니다.

#### Data augmentation
[25]에서 영감을 받아, 우리는 훈련 중 무작위 이동 및 자르기 증강을 도입하여 전경 객체가 항상 자르기 창에 포함되도록 합니다. 이 과정은 Fig. 4의 하단 행에 설명되어 있습니다. 훈련 및 추론 중 이 증강 방법을 적용하면 생성된 결과의 현실성이 현저히 향상됩니다. 정량적 결과는 섹션 4.4에 제공됩니다.

### 3.3.2 훈련

#### Content Adaptor pretraining
우리는 먼저 content adaptor를 사전 훈련하여 객체의 의미론을 유지하면서 이미지 임베딩을 텍스트 임베딩으로 매핑합니다. 첫 번째 단계에서는 시퀀스-투-시퀀스 번역 작업에서 content adaptor를 최적화하여 이미지 임베딩을 다중 모드 공간으로 투영하는 법을 학습합니다. 훈련 중에 $C_t$, $C_i$는 고정되고, 번역기는 필터링된 LAION 데이터셋 [41]에서 3,203,338개의 이미지-캡션 쌍으로 훈련됩니다.

주어진 입력 이미지 임베딩 $\tilde{E}$에서 텍스트 임베딩 $E$를 목표로 사용하여 이 번역 작업의 목적 함수는 다음과 같이 정의됩니다:


$$L_{dist} = \| T(\tilde{E}) - E \|_1,$$


여기서 $T(\cdot)$는 content adaptor입니다.

그러나 첫 번째 단계 최적화에서 얻어진 다중 모드 임베딩은 주로 입력 이미지의 고수준 의미론을 포함하지만, 질감 세부 사항은 많이 포함하지 않습니다. 따라서 우리는 더 나은 외관 보존을 위해 적응형 임베딩 $\hat{E}$를 얻기 위해 $\tilde{E}$를 추가로 세밀하게 조정합니다.

#### Content Adaptor Fine-tuning

우리는 객체의 인스턴스 수준 속성을 유지하는 적응형 임베딩을 생성하기 위해 content adaptor를 추가로 최적화합니다. 사전 훈련 후, 우리는 content adaptor를 사전 학습된 diffusion 프레임워크에 삽입하여 attention blocks에 컨텍스트로 적응형 임베딩을 제공합니다. 그런 다음 diffusion model은 고정되고 adaptor는 다음을 사용하여 훈련됩니다:


$$
L_{adapt} = \mathbb{E}_{T, \epsilon \sim \mathcal{N}(0, 1)} [ \| \epsilon - \epsilon_\theta (I_t \circ M, t, T(\tilde{E})) \|_2^2 ],
$$

여기서 content adaptor $T(\cdot)$가 최적화됩니다. 이 과정은 Pixabay에서 필터링된 합성 데이터셋에서 훈련되며, 467,280개의 전경 및 배경 이미지 쌍을 훈련에 사용하고 25,960개의 쌍을 검증에 사용합니다.

#### Generator Fine-tuning

content adaptor의 두 단계 훈련이 완료된 후, content adaptor를 고정하고 사전 학습된 가중치로 텍스트-이미지 diffusion model을 초기화한 후 생성기 모듈을 훈련합니다. 이 과정에서 자르기 및 이동 증강이 적용됩니다. Latent Diffusion model [38]을 기반으로 생성기 모듈에서의 목적 손실은 다음과 같이 정의됩니다:


$$L_{gen} = \mathbb{E}_{\hat{E}, \epsilon \sim \mathcal{N}(0, 1)} [ \| \epsilon - \epsilon_\theta (I_t \circ M, t, \hat{E}) \|_2^2 ],$$


여기서 $I$는 입력 이미지이고, $I_t$는 시점 $t$에서 $I$의 노이즈 버전이며, $\epsilon_\theta$는 최적화되는 디노이징 모델을 나타냅니다. 이미지 합성의 설정에 텍스트 가이드 생성 작업을 적응시키기 위해, 우리는 매 시점마다 이미지에 입력 마스크를 적용합니다.