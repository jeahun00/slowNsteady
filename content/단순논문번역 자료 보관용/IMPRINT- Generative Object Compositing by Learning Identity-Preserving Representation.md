
# Abstract

Generative object compositing은 조합적 이미지 편집을 위한 유망한 새로운 방법으로 부상하고 있습니다. 그러나 객체의 정체성을 유지해야 하는 요구사항은 큰 도전 과제가 되어 대부분의 기존 방법의 실용적인 사용을 제한하고 있습니다. 이에 대응하여 본 논문은 정체성 유지 학습과 합성 학습을 분리하는 두 단계 학습 프레임워크로 훈련된 새로운 diffusion-based generative model인 IMPRINT를 소개합니다. 첫 번째 단계는 context-agnostic, identity-preserving 사전 학습을 목표로 하여 object encoder가 시각 불변성이며 세부 사항 보존을 촉진하는 embedding을 학습할 수 있게 합니다. 그 다음 단계에서는 이 표현을 활용하여 배경에 합성된 객체의 원활한 조화를 학습합니다. 또한, IMPRINT는 사용자가 합성 과정을 제어할 수 있는 shape-guidance 메커니즘을 통합하고 있습니다. 광범위한 실험을 통해 IMPRINT가 정체성 유지와 합성 품질 측면에서 기존 방법과 다양한 기준을 크게 능가함을 입증했습니다. 프로젝트 페이지: https://song630.github.io/IMPRINT-Project-Page/

# 1. Introduction

이미지 합성은 참조 객체를 배경과 결합하여 일관되고 현실적인 이미지를 만드는 예술로, diffusion models (DM) [13, 32, 33, 36]의 등장과 함께 획기적인 발전을 이루었습니다. 이러한 모델들은 두 가지 중요한 측면인 정체성(ID) 유지와 배경 조화를 중심으로 하는 새로운 과제인 generative object compositing의 등장을 촉진시켰습니다. 목표는 합성 이미지에서 객체가 정체성을 유지하면서도 배경과의 원활한 통합을 위해 색상과 기하학적 형태를 조정하는 것입니다. 기존 방법들 [27, 41, 46]은 generative compositing에서 인상적인 능력을 보여주지만, ID 유지 또는 문맥 일관성에서 종종 실패합니다.
최근 연구들 [41, 46]은 ID 유지와 배경 조화의 균형을 맞추는 데 어려움을 겪고 있습니다. 이러한 방법들은 공간적 조정에서 진전을 이루었으나, 주로 범주적 정보보다는 세부 정보를 포착합니다. TF-ICON [27]과 두 개의 동시 연구 [4, 48]는 주제 충실도를 향상시켰지만, 배경 통합을 위한 자세 및 시야 변화의 제한으로 인해 실제 적용 가능성을 제한하고 있습니다.

ID 유지와 자세 조정을 위한 배경 정렬 간의 균형 문제를 해결하기 위해, 우리는 IMPRINT라는 새로운 두 단계 합성 프레임워크를 도입합니다. IMPRINT는 ID 유지에서 뛰어난 성능을 발휘하며, 이전 연구와 달리 합성 과정을 ID 유지와 배경 정렬 단계로 분리합니다. 첫 번째 단계는 새로운 context-agnostic ID-preserving 학습을 포함하며, 여기서 이미지 인코더는 세부 사항 각인을 위해 중요한 view-invariant 특징을 학습하도록 훈련됩니다. 두 번째 단계는 첫 번째 단계에서 얻은 강력한 ID-preserving 표현을 활용하여 객체와 배경을 조화롭게 통합하는 데 중점을 둡니다. 이 이분법적 접근은 객체 세부 사항의 전례 없는 충실도를 제공하면서도 색상과 기하학적 조화를 용이하게 합니다.

우리의 기여는 다음과 같이 요약될 수 있습니다:
- 우리는 포괄적인 실험을 통해 우수한 외관 보존을 입증하는 새로운 context-agnostic ID-preserving 학습을 도입합니다.
- 우리의 두 단계 프레임워크는 ID 유지와 배경 정렬 작업을 명확히 분리하여 현실적인 합성 효과를 가능하게 합니다.
- 우리는 모델에 마스크 제어를 통합하여 shape guidance와 생성 유연성을 향상시킵니다.
- 우리는 외관 보존에 대한 광범위한 연구를 수행하여 ID 유지에 영향을 미치는 다양한 요소들, 예를 들어 이미지 인코더, 다중 뷰 데이터셋, 학습 전략 등에 대한 통찰을 제공합니다.

# 2. Related Work
## 2.1. Image Compositing
이미지 합성은 주어진 전경 이미지의 객체를 배경 이미지에 삽입하는 참조 안내 이미지 편집에서 도전적인 작업입니다. 생성된 합성 이미지는 원래 객체의 외관을 보존하면서 현실적으로 보여야 합니다. 이전 연구들은 종종 이 문제의 개별 측면에 초점을 맞추는데, 예를 들어 기하학적 수정 [2, 28], 이미지 조화 [6, 7, 51], 이미지 매팅 [50] 및 그림자 생성 [29] 등이 있습니다.

Lin 등 [28]은 객체와 배경 이미지 간의 기하학적 불일치를 수정하기 위해 반복적인 GAN 기반 프레임워크를 제안합니다. 그들의 모델에서는 STNs [18]가 생성기 네트워크에 통합되어 일련의 왜곡 업데이트를 예측합니다. 이 연구는 합성 이미지에서 장면 기하학의 현실성을 향상시킬 수 있지만, 가구를 실내 장면에 삽입하는 데만 제한됩니다. Azadi 등 [2]은 이진 합성에 집중하여 두 객체 간의 상호작용을 포착하기 위해 합성-분해 네트워크를 사용합니다. 이미지 조화는 입력 객체와 배경 이미지 간의 불일치를 최소화하는 것을 목표로 합니다. 전통적인 방법 [24, 46]은 주로 색상 통계를 얻은 다음 이 정보를 전경과 배경 간에 전송하는 것에 집중합니다. 최근 연구들은 딥 뉴럴 네트워크를 통해 이 문제를 해결하려고 합니다. [6, 7]은 이미지 조화를 도메인 적응 문제로 재구성하고, [19]는 이를 표현 융합 문제로 변환하여 자기 지도 학습을 활용합니다.

그림자 합성은 이전 이미지 합성 방법에서 종종 간과되지만 현실적인 합성 이미지를 생성하는 데 필수적입니다. SGRNet [16]은 이 작업을 그림자 마스크 생성 단계와 그림자 채우기 단계로 나눕니다. SSN [43]은 소프트 그림자에 중점을 두고 그림자 생성을 위한 단서로서 주변 폐색 맵을 예측합니다.

이미지 합성의 여러 하위 문제를 동시에 해결하는 연구들도 있습니다. Chen 등 [5]은 기하학적, 색상 및 경계 일관성을 갖춘 그럴듯한 이미지를 생성하기 위해 여러 네트워크 모듈을 포함하는 시스템을 개발했습니다. 그러나 생성된 객체의 자세는 마스크 입력에 의해 제한되며, 모델은 동물과 같은 비강성 객체에 일반화할 수 없습니다. 우리가 제안한 모델은 또한 통합된 방식으로 이미지 합성 문제를 처리하여 배경과 조화롭고 기하학적으로 일관된 전경 객체를 생성하면서 새로운 뷰와 그림자를 합성합니다.

# 2.2. Subject-Driven Image Generation
Subject-driven image generation, 즉 새로운 맥락에서 주제를 생성하는 작업은 종종 텍스트 프롬프트를 기반으로 주제 속성을 맞춤화하는 과정을 포함합니다. Diffusion models을 기반으로 한 [7, 17]의 연구는 객체 표현을 위한 placeholder 단어를 사용하여 고충실도의 맞춤화를 가능하게 하는 기술로 이어졌습니다. 후속 연구들 [19, 26, 34, 35]은 새로운 개념 학습을 위해 사전 훈련된 text-to-image 모델을 미세 조정하는 방법으로 이를 확장했습니다. 이러한 발전은 주제 교체 [8], open-world generation [22], 비강성 이미지 편집 [2]과 같은 다양한 응용을 가능하게 했습니다. 그러나 이러한 방법들은 대개 추론 시 미세 조정 또는 여러 주제 이미지를 필요로 하여 실용성이 제한됩니다. 반면, 우리의 프레임워크는 실세계 데이터의 광범위한 스펙트럼에 대해 빠르고 배경을 보존하는 접근 방식을 제공합니다.
# 3. Approach
제안된 객체 합성 프레임워크인 IMPRINT는 그림 2에 요약되어 있습니다. 공식적으로, 객체 $I_{obj} \in \mathbb{R}^{H \times W \times 3}$, 배경 $I_{bg} \in \mathbb{R}^{H \times W \times 3}$, 객체를 배경에 합성할 위치와 크기를 나타내는 마스크 $M \in \mathbb{R}^{H \times W}$가 주어지면, 합성 모델 $C$를 학습하여 합성 이미지 $I_{out} = C(I_{obj}, I_{bg}, M) \in \mathbb{R}^{H \times W \times 3}$를 얻는 것이 목표입니다. 이상적인 결과는 시각적으로 일관되고 자연스러운 $I_{out}$이며, 즉 $C$는 합성된 객체가 $I_{obj}$의 정체성을 유지하고, $I_{bg}$의 기하학적 구조에 맞추어 배경에 원활하게 융합되도록 보장해야 합니다.

이 섹션에서는 접근 방식을 확장하여 설명합니다. 사전 훈련된 text-to-image diffusion models을 활용하기 위해, 우리는 텍스트 인코딩 브랜치를 대체할 새로운 이미지 인코더를 설계하여 참조 객체로부터 훨씬 더 풍부한 정보를 유지합니다 (섹션 3.1 참조). 기존 연구와는 달리, 우리의 파이프라인은 객체 충실도와 기하학적 변화를 동시에 보장하기 위해 작업을 두 가지 특화된 하위 작업으로 이분화합니다. 첫 번째 단계는 context-agnostic ID-preserving 작업을 정의하며, 여기서 이미지 인코더는 일반 객체의 통합된 표현을 학습하도록 훈련됩니다 (섹션 3.1). 두 번째 단계는 주로 이미지 합성을 위한 생성기를 훈련시킵니다 (섹션 3.2). 추가적으로, 프레임워크의 세부 보존 능력에 기여하는 다양한 측면을 탐구합니다. 섹션 3.3에서는 페어링된 데이터 수집 과정을 논의하고, 섹션 3.4에서는 우리의 학습 전략을 자세히 설명합니다.

## 3.1. Context-Agnostic ID-preserving Stage
기존 방법과는 달리, 우리는 ID 유지를 돕기 위해 훈련의 첫 번째 단계로 감독된 객체 뷰 복원 작업을 도입합니다. 이 작업의 동기는 다음의 주요 관찰에 기반합니다:

- 세부 사항 보존을 성공적으로 개선한 기존 연구들 [4, 27, 48]은 기하학적 조화에서 제한되며, 복사-붙여넣기 행동을 보여주는 경향이 있습니다.
- ID 유지와 이미지 합성 간에는 근본적인 트레이드오프가 있습니다. 객체는 배경과 더 잘 맞추기 위해 색상, 조명, 기하학적 측면에서 변경될 것으로 예상되지만, 동시에 객체의 원래 자세, 색조, 조명 효과는 모델에 의해 기억되며 외관을 정의합니다.
- 다중 뷰 데이터는 ID 유지에 중요한 역할을 하지만, 이러한 데이터셋을 획득하는 것은 비용이 많이 듭니다. 대부분의 대규모 다중 뷰 데이터셋 [5, 47]은 합성을 위한 충분한 맥락 정보를 결여하고 있습니다. 이들은 배경이 전혀 없거나, 배경 영역이 너무 제한적입니다.

위의 통찰을 바탕으로, 우리는 과제의 공식 정의를 제공합니다 (그림 2a에 묘사됨). 두 뷰 $I_{v1}, I_{v2}$와 해당 마스크 $M_{v1}, M_{v2}$를 갖는 객체가 주어졌을 때, 배경이 제거되고 세그먼트된 객체 쌍은 $\hat{I}_{v1} = I_{v1} \otimes M_{v1}$, $\hat{I}_{v2} = I_{v2} \otimes M_{v2}$로 표기됩니다. 우리는 뷰 합성 모델 $\mathcal{S} = \{ \mathcal{E}_u, \mathcal{G}_\theta \}$를 구축하여 $\hat{I}_{v1}$를 조건으로 목표 뷰 $\hat{I}_{v2}$를 생성합니다. 여기서 $\mathcal{E}_u$는 이미지 인코더이고 $\mathcal{G}_\theta$는 $\theta$로 매개변수화된 UNet 백본입니다.

이미지 인코더 $\mathcal{E}_u$는 사전 훈련된 DINOV2 [29]와 [41]을 따른 content adapter로 구성됩니다. DINOV2는 이전 모델들 [15, 31, 38]을 능가하는 SOTA ViT 모델로, 참조 기반 생성에 매우 표현력 있는 시각적 특징을 추출합니다. content adapter는 이미지와 텍스트 임베딩 공간 간의 도메인 간격을 연결하여 사전 훈련된 T2I 모델의 활용을 가능하게 합니다.

이미지 디코더 $\mathcal{G}_\theta$는 Stable Diffusion [33]의 조건부 denoising autoencoder $\mathcal{G}_\theta$를 받아 훈련 중 디코더를 미세 조정합니다. 목표 함수는 다음과 같이 정의됩니다 (참조 [33]):
$$\mathcal{L}_{id} = \mathbb{E}_{\hat{I}_{v1}, \hat{I}_{v2}, t, \epsilon} \left[ \left\| \epsilon - \mathcal{G}_\theta \left( \hat{I}_{v2}, t, \mathcal{E}_u \left( \hat{I}_{v1} \right) \right) \right\|_2^2 \right],$$
여기서 $\mathcal{L}_{id}$는 ID 유지 손실이며, $\epsilon \sim \mathcal{N}(0, 1)$입니다. 이미지 인코더 $\mathcal{E}_u$와 $\mathcal{G}_\theta$의 디코더 블록은 이 과정에서 최적화됩니다. 직관적으로, 이 작업을 위해 훈련된 인코더는 항상 뷰 불변의 표현을 추출하면서 다양한 뷰에 걸쳐 공유되는 ID 관련 세부 사항을 유지합니다. 이 단계의 정성적 결과는 섹션 4.7에 나타나 있습니다. 이전 뷰 합성 연구들 [25]과는 달리, 우리의 context-agnostic ID-preserving 단계는 3D 정보 (예: 카메라 매개변수)를 조건으로 요구하지 않으며, 우리는 주로 ID 보존에 초점을 맞추고 있습니다. 기하학적 일관성은 배경과의 일관성보다 중요하게 다루어집니다 (이는 두 번째 단계에서 처리됩니다). 따라서 이미지 인코더만 다음 단계로 넘어갑니다.

## 3.2. Compositing Stage
그림 2b는 합성 작업을 위해 훈련된 두 번째 단계의 파이프라인을 보여줍니다. 이 단계는 세밀하게 조정된 이미지 인코더 $\mathcal{E}_u$와 ID-preserving 표현을 조건으로 하는 생성기 $\mathcal{G}_\phi$ (φ로 매개변수화됨)로 구성됩니다.

간단한 접근 방식은 뷰 합성 단계를 무시하고 인코더와 생성기를 단일 단계 프레임워크에서 공동으로 훈련하는 것입니다. 그러나 이러한 단순한 시도에서는 두 가지 측면에서 품질 저하가 발생함을 발견했습니다 (섹션 4.7 참조):

- DINOV2가 이 단계에서 훈련될 때, 모델은 객체를 원래 뷰와 매우 유사한 뷰로 합성하는 더 빈번한 복사-붙여넣기와 같은 행동을 나타냅니다.
- 객체 중심 다중 뷰 데이터셋 (예: MVImgNet [47])이 훈련 세트에 포함될 때, 모델은 더 많은 아티팩트를 생성하고 이러한 데이터셋에서 배경 정보가 부족하여 블렌딩 결과가 저하됩니다.

위의 문제를 극복하기 위해, 우리는 두 번째 단계에서 이미지 인코더 (즉, DINOV2)의 백본을 고정하고 훈련 세트를 신중하게 수집합니다 (섹션 3.3 참조).

이 단계에서는 생성기의 백본으로 사전 훈련된 T2I 모델을 활용합니다. 이 모델은 배경 $I_{bg}$, 거친 마스크 $M$을 입력으로 사용하며, ID-preserving 객체 토큰 $\hat{\mathcal{E}}_u = \mathcal{E}_u(I_{obj})$를 조건으로 합니다. 여기서 $I_{obj}$는 마스크된 객체 이미지를 나타냅니다. 생성은 객체 토큰을 $\mathcal{G}_\phi$의 교차 주의 레이어에 주입함으로써 안내됩니다. 거친 마스크는 그림자 합성 및 객체와 주변 객체 간의 상호작용도 허용합니다.

$\hat{{E}}_u$는 이미 구조화된 뷰 불변 객체 세부 사항을 포함하고 있으므로, 색상 및 기하학적 조정은 더 이상 ID 보존 노력에 의해 제한되 않습니다. 이러한 자유는 합성에서 더 큰 변화를 허용합니다.

이 단계의 목표 함수는 다음과 같이 정의됩니다:
$$\mathcal{L}_{comp} = \mathbb{E}_{I_{obj}, I_{bg}^*, M, t, \epsilon} \left[ M \left\| \epsilon - \mathcal{G}_\phi \left( I_{bg}^*, t, \hat{\mathcal{E}}_u \right) \right\|_2^2 \right],$$
여기서 $\mathcal{L}_{comp}$는 합성 손실이며, $I_{bg}^*$는 목표 이미지입니다. $\mathcal{G}_\phi$와 어댑터가 최적화됩니다.

**The Background-blending Process**
객체와 배경 사이의 전환 영역이 부드럽게 유지되도록 하기 위해, 우리는 배경 혼합 전략을 채택합니다. 이 과정은 그림 3에 묘사되어 있습니다.

**Shape-guided Controllable Compositing**
형태 안내 가능한 제어 합성은 거친 마스크를 그려 생성된 객체의 자세와 뷰를 보다 실질적으로 안내할 수 있습니다. 그러나 대부분의 이전 연구들 [4, 27, 41]에는 이러한 제어가 없습니다. 제안된 모델에서는 [43]을 따르며, 마스크는 네 가지 정밀도 수준으로 정의됩니다 (부록 참조). 가장 거친 마스크는 바운딩 박스입니다. 여러 수준의 마스크를 통합함으로써 사용자가 더 정밀한 마스크를 선호하는 실제 시나리오를 복제할 수 있습니다. 결과는 그림 1에 나타나 있습니다.

## 3.3. Paired Data Generation
데이터셋의 품질은 ID 유지와 자세 변형을 개선하는 또 다른 중요한 요소입니다. [4]에 의해 입증된 바와 같이, 다중 뷰 데이터셋은 생성 충실도를 크게 향상시킬 수 있습니다. 실제로, 우리는 이미지 데이터셋 (Pixabay), 파놉틱 비디오 세분화 데이터셋 (YoutubeVOS [44], VIPSeg [28] 및 PPR10K [23]) 및 객체 중심 데이터셋 (MVImgNet [47] 및 Objaverse [5])의 조합을 사용합니다. 이들은 다양한 학습 단계에 통합되어 자기 지도 학습에서 다양한 처리 절차와 연관됩니다.

우리가 수집한 이미지 데이터셋은 고해상도 및 풍부한 배경 정보를 가지고 있어, 더 나은 합성을 위해 두 번째 단계에서만 사용됩니다. [41, 46]에 영감을 받아, 객체 합성에서 조명 및 기하학적 변화를 시뮬레이션하기 위해, 우리는 다음과 같은 증강 파이프라인을 설계합니다:
 $$\hat{I}_{obj} = \mathcal{P}(\mathcal{T}(I_{obj})),$$
여기서 $\mathcal{T}$는 아핀 변환이고, $\mathcal{P}$는 색상 및 조명 변형이며, [16]의 룩업 테이블에 의해 지원됩니다. 변형된 객체 $\hat{I}_{obj}$는 입력으로 사용되며, 원래 객체를 포함한 자연 이미지 $I_{bg}^*$는 목표로 사용됩니다.

비디오 세분화 데이터셋은 일반적으로 저해상도 및 모션 블러로 인해 생성 품질이 저하됩니다. 그럼에도 불구하고, 이들은 조명, 기하학, 뷰 및 비강성 자세 변형이 자연스럽게 다른 객체 쌍을 제공합니다. 결과적으로, 이들은 두 번째 단계에서도 사용됩니다. 그림 4에 나타난 바와 같이, 각 학습 쌍은 인스턴스 수준의 세분화 레이블이 있는 하나의 비디오에서 나옵니다. 두 개의 서로 다른 프레임이 무작위로 샘플링되며, 하나는 목표 이미지로 사용되고 객체는 증강된 입력의 다른 프레임에서 추출됩니다.

객체 중심 데이터셋은 비디오 세분화 데이터셋보다 훨씬 더 큰 규모를 제공하며, 더 복잡한 객체 세부 사항을 제공합니다. 그러나 이 데이터셋에 사용 가능한 배경 정보가 제한적이기 때문에 첫 번째 단계에서만 사용됩니다. 학습 중, 각 쌍 $I_{v1}, I_{v2}$는 동일한 비디오에서 무작위로 샘플링되며, $|v1 - v2| \leq n$인 경우 $n$은 시간 샘플링 창입니다. 경험적으로, $n = 7$이 충실도와 품질 사이의 균형을 이루는 것을 관찰했습니다.

## 3.4. Training Strategies
모든 이전 (또는 동시) 학습이 필요 없는 방법 [4, 41, 46, 48]은 DINOV2 또는 CLIP을 사용하여 frozen transformer 기반 이미지 인코더를 사용합니다. 그러나 인코더를 고정하면 객체 세부 사항을 추출하는 능력이 제한됩니다: i) CLIP은 객체의 의미론적 특징만 인코딩합니다; ii) DINOV2는 이미지 검색을 기반으로 구축된 데이터셋에서 학습되며, 완전히 동일하지 않은 객체를 동일한 인스턴스로 취급할 수 있습니다. 이 문제를 극복하기 위해, 우리는 합성을 위해 인코더를 미세 조정하여 인스턴스 수준의 특징을 추출할 수 있도록 합니다.

위에서 언급한 인코더의 광범위한 규모로 인해, 과적합에 취약합니다. 적절한 학습 전략을 구현하면 학습 과정을 효과적으로 안정화하고 ID 보존을 향상시킬 수 있습니다. 이를 위해, 우리는 새로운 학습 방식인 Sequential Collaborative Training을 설계합니다.

더 구체적으로, 객체 합성 단계는 두 단계로 나뉩니다:
1) 첫 $n$ epochs에서 어댑터에 대해 $4 \times 10^{-5}$의 더 큰 학습률을 할당하고, UNet에는 $4 \times 10^{-6}$의 더 작은 학습률을 할당합니다.
2) 다음 $n$ epochs에서 이 두 구성 요소의 학습률을 교환합니다 (그리고 학습이 끝납니다). 이 전략은 각 단계에서 한 구성 요소를 집중적으로 학습하고, 다른 구성 요소는 낮은 학습률로 동시에 학습하여 변경된 도메인에 적응하도록 합니다; 생성기는 품질을 보장하기 위해 마지막에 학습됩니다.