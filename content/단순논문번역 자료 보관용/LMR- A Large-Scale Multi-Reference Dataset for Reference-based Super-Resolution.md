
# Abstract

참조 기반 초해상도(Reference-based Super-Resolution, RefSR)는 단일 이미지 초해상도(Single Image Super-Resolution, SISR)와 비교할 때, 유사한 고품질 이미지를 참조함으로써 우수한 결과를 달성하는 것으로 널리 알려져 있습니다. 직관적으로 참조 이미지가 많을수록 성능이 향상됩니다. 그러나 이전의 RefSR 방법들은 모두 단일 참조 이미지 훈련에 초점을 맞추었으며, 테스트나 실제 응용에서는 여러 참조 이미지가 종종 사용 가능합니다. 이러한 훈련-테스트 불일치의 근본 원인은 공개된 다중 참조 초해상도 훈련 데이터셋의 부재이며, 이는 다중 참조 초해상도에 대한 연구를 크게 저해합니다. 이를 해결하기 위해, 우리는 대규모 다중 참조 초해상도 데이터셋을 구축하였으며, 이를 LMR이라 명명하였습니다. 이 데이터셋은 300×300 크기의 훈련 이미지 112,142 그룹을 포함하고 있으며, 이는 기존 최대 RefSR 데이터셋의 10배에 해당합니다. 이미지 크기도 훨씬 큽니다. 더욱이, 각 그룹은 서로 다른 유사도 수준의 5개의 참조 이미지로 구성되어 있습니다. 또한, 우리는 다중 참조 초해상도를 위한 새로운 기준 방법인 MRefSR을 제안합니다. 이는 Multi-Reference Attention Module (MAM)과 Spatial Aware Filtering Module (SAFM)을 포함하여, 임의의 수의 참조 이미지에 대한 특징 융합과 융합된 특징 선택을 수행합니다. 제안된 MRefSR은 양적 및 질적 평가에서 최신 기법들보다 현저한 개선을 달성합니다. [GitHub 링크](https://github.com/wdmwhh/MRefSR)

# 1. Introduction

Single image super-resolution (SISR)은 저해상도(Low-Resolution, LR) 이미지를 고해상도(High-Resolution, HR) 이미지로 복원하는 것입니다 \[11\]. SISR은 감시 \[39\], 천문학 \[8\], 의료 영상 \[7\], 영화 및 텔레비전 \[23, 14\], 기타 산업 \[26, 37, 33\]에서 광범위하게 응용됩니다. 딥러닝의 발전과 함께, SISR은 지난 몇 년간 큰 발전을 이루었습니다 \[4, 5, 16, 18, 13, 40, 31, 19, 3, 22\]. SISR과 비교하여, reference-based super-resolution (RefSR)은 추가적인 유사한 HR 참조 이미지들에서 텍스처를 활용할 수 있어 종종 더 나은 성능을 달성합니다.

최근 RefSR 방법들이 보여준 유망한 결과들 \[28, 36, 42, 41, 27, 29, 12, 20, 34\]로 인해, 이는 더 많은 연구 관심을 끌고 있습니다. 그러나, 이전의 모든 RefSR 방법들은 단일 참조 이미지를 사용한 훈련에 초점을 맞추었지만, 실제 응용이나 테스트에서는 종종 여러 참조 이미지가 사용 가능합니다. 우리가 아는 한, 현재 유일하게 공개된 RefSR 훈련 데이터셋은 CUFED5 \[41, 32\]로, 160×160의 작은 해상도를 가진 11,871개의 이미지 쌍만을 포함하고 있습니다. 더 중요한 것은, 각 LR 입력 이미지에 대해 단 하나의 참조 이미지만 존재한다는 점입니다. 그러나, 실제 응용에서는 종종 여러 참조 이미지가 사용됩니다. 예를 들어, CUFED5 테스트 세트에는 126개의 입력 이미지가 있으며, 각 입력 이미지는 서로 다른 유사도 수준의 5개의 참조 이미지를 가지고 있습니다. 마찬가지로, 우리는 실제 테스트 케이스에 대해 여러 참조 이미지를 쉽게 찾을 수 있습니다. 유일하게 공개된 훈련 데이터셋의 제한으로 인해, 이전의 RefSR 방법들은 테스트나 실제 응용에서 여러 참조 이미지를 잘 활용하지 못합니다. 이전의 RefSR 방법들은 여러 참조 이미지를 결합하여 하나의 참조 이미지로 만들어 단일 참조 이미지로 훈련된 모델에 맞추는 방식을 사용합니다. 그러나 참조 이미지의 해상도가 너무 큰 경우, 이러한 테스트 방식은 GPU 메모리를 소진시킬 것입니다. 또한, 여러 참조 이미지 간의 관계는 효과적으로 모델링되지 않습니다. 따라서, 이는 다중 참조 이미지를 위해 특별히 설계된 방법보다 훨씬 열악합니다. 따라서, 다중 참조 RefSR 훈련 데이터셋과 간단하지만 효과적인 다중 참조 RefSR 방법이 필요합니다.

이 논문에서는 LMR이라 명명된 대규모 다중 참조 RefSR 데이터셋을 제안합니다. LMR의 훈련 세트는 300×300 크기의 훈련 이미지 112,142 그룹으로 구성되어 있으며, 각 그룹에는 서로 다른 유사도 수준의 5개의 참조 이미지가 포함되어 있습니다. LMR 훈련 데이터셋은 CUFED5와 비교하여 10배 많은 이미지를 포함하고 있으며, 이미지 크기도 훨씬 큽니다. 이렇게 충분히 큰 훈련 데이터셋은 모델의 일반화 능력을 향상시키는 데 도움이 될 것입니다. 우리는 이 훈련 데이터셋이 다중 참조 이미지를 포함한 첫 번째 RefSR 훈련 데이터셋으로서 RefSR 연구를 크게 촉진할 것이라고 믿습니다. 한편, LMR의 테스트 세트는 142 그룹의 이미지를 가지고 있으며, 각 그룹은 2~6개의 참조 이미지를 포함하고 있습니다. 테스트 이미지의 한 변 길이는 800에서 1600까지 다양합니다.

LMR의 도움으로, 우리는 다중 참조 RefSR을 위한 새로운 기준 방법인 MRefSR을 제안합니다. 먼저, 우리는 임의의 수의 참조 이미지로부터 특징 융합을 위한 Multi-Reference Attention Module (MAM)을 개발합니다. 우리는 LR 입력 특징을 쿼리로 취급하며, 후보 키와 값은 서로 다른 참조 이미지에 대응하는 정렬된 참조 특징으로부터 생성됩니다. 그런 다음, 서로 다른 정렬된 참조 특징 간의 주의(attention)를 통해 다양한 참조 이미지의 특징을 융합합니다. 두 번째로, 모든 LR 특징 지점이 참조 특징과 잘 일치하지 않기 때문에, 우리는 융합된 특징 선택을 위한 Spatial Aware Filtering Module (SAFM)을 사용합니다. 그림 1에서 보이듯이, 우리의 MRefSR은 여러 참조 이미지로부터 시각적으로 만족스러운 세부 사항을 생성하기 위해 정보를 효과적으로 활용합니다. 요약하자면, 우리의 기여는 다음 세 가지로 요약됩니다:

- 우리는 112,142 그룹의 300×300 훈련 이미지를 포함하고 각 그룹에 입력 이미지에 대한 5개의 참조 이미지가 있는 첫 번째 다중 참조 RefSR 데이터셋인 LMR을 기여합니다. 이 데이터셋은 단일 참조에서 다중 참조 이미지로 RefSR 연구를 가능하게 하고 RefSR 연구 분야의 발전을 크게 촉진할 것입니다.

- 우리는 임의의 수의 참조 이미지로부터 특징 융합을 위한 Multi-Reference Attention Module과 융합된 특징 선택을 위한 Spatial Aware Filtering Module을 사용하는 새로운 다중 참조 기준 RefSR 방법인 MRefSR을 제안합니다. 우리의 방법은 여러 참조 이미지 간의 관계를 효과적으로 학습하고 이를 최대한 활용합니다. 이는 또한 다중 참조 데이터셋 LRM 덕분입니다.

- 우리는 제안된 LMR의 우수성과 다중 참조 RefSR 방법의 잠재력을 입증하는 광범위한 실험을 수행합니다. 우리의 방법은 양적 및 질적 평가에서 최신 기법들보다 현저한 개선을 달성합니다.


# 2. Related Work

## 2.1. Reference-based Image Super-Resolution

RefSR은 점차 떠오르는 연구 분야가 되고 있습니다. SISR과 비교하여, RefSR은 유사한 내용의 추가 HR 참조 이미지의 정보를 활용할 수 있기 때문에 더 유리합니다. SRNTT \[41\]은 참조 이미지로부터 SR 이미지를 복원하기 위해 다중 스케일 적응 텍스처 전이를 수행하는 종단간 네트워크 구조를 제안하였습니다. 이후, TTSR \[35\]은 다중 스케일 참조 특징을 병합하기 위해 교차 스케일 특징 통합 방법을 적용하였습니다. MASA \[20\]는 계산 복잡성을 줄이기 위해 거칠게부터 정교하게 패치 매칭 방식을 설계하였습니다. 결과적으로, $C^2$-Matching \[12\]은 교사-학생 상관 증류와 동적 DCN \[2, 43\] 집계 모듈을 통해 참조 특징의 LR 특징에 대한 사전 오프셋을 보다 정확하게 얻었습니다. AMSA \[34\]는 다중 스케일 집계 및 거칠게부터 정교하게 패치 매칭을 도입하여 $C^2$-Matching의 증분 확장을 수행하였습니다. Huang 등 \[10\]도 $C^2$-Matching 모델을 사용하였지만, 추가적인 SISR 네트워크를 도입하여 텍스처 전이와 초해상도를 분리함으로써 네트워크 파라미터가 훨씬 커지고 추론이 훨씬 느려졌습니다. 최근에는, RRSR \[38\]과 DATS \[1\]도 성능을 향상시키기 위해 상호 학습과 변환자를 도입하였습니다. 이전 방법들이 큰 발전을 이루었지만, 모든 방법은 유일하게 사용 가능한 훈련 데이터셋인 CUFED5의 제한으로 인해 단일 참조 이미지만을 사용한 연구 탐구에 초점을 맞추고 있습니다.

## 2.2. RefSR Datasets

우리의 지식에 따르면, RefSR 연구에서 일반적으로 사용되는 다섯 가지 데이터셋은 Sun80 \[28\], Urban100 \[9\], Manga109 \[21\], WR-SR \[12\] 및 CUFED5 \[41, 32\]입니다. 그러나 처음 네 개는 모두 테스트 세트입니다. Sun80 데이터셋은 80개의 자연 이미지를 포함하고 있으며, 각 이미지에는 20개의 웹 검색 참조 이미지가 있지만, 이러한 참조 이미지는 해당 LR 입력과 매우 유사하지 않으므로 RefSR의 테스트 세트로 적합하지 않습니다. Urban100 데이터셋은 참조가 없는 100개의 건물 이미지를 포함합니다. 건물 이미지 내의 자가 유사성으로 인해, 해당 LR 이미지는 일반적으로 참조 이미지로 취급됩니다. Manga109 데이터셋은 참조가 없는 109개의 만화 이미지를 포함합니다. Manga109의 모든 이미지는 동일한 카테고리(만화 표지)이기 때문에, 이전 방법들은 데이터셋에서 하나의 HR 이미지를 참조 이미지로 무작위로 사용합니다. WR-SR 데이터셋은 더 다양한 카테고리로, 각 타겟 이미지에 웹 검색 참조 이미지가 동반된 80개의 이미지 쌍을 포함합니다. CUFED5 \[41, 32\]는 훈련 세트로, 160×160의 작은 해상도를 가진 11,871개의 이미지 쌍을 포함하며, 각 이미지 쌍에 LR 입력에 대한 단 하나의 참조 이미지만 있습니다. CUFED5 테스트 세트에는 126개의 입력 이미지가 있으며, 각 입력 이미지에는 서로 다른 유사도 수준의 5개의 참조 이미지가 있습니다. 최근 Wang 등 \[30\]은 듀얼 카메라 초해상도를 위한 새로운 데이터셋 CameraFusion을 제안하였으며, 131개의 훈련 이미지 쌍과 15개의 테스트 이미지 쌍을 포함합니다. 그러나 듀얼 카메라에 의해 캡처된 이미지 쌍은 RefSR 작업에 너무 이상적이며, 데이터셋의 수가 너무 적습니다. 이 논문에서는 RefSR 연구의 요구를 더 잘 충족시키기 위해, 대규모 다중 참조 RefSR 데이터셋인 LMR을 제안합니다.

# 3. Approach

이 절에서는 먼저 제안된 대규모 다중 참조 RefSR 데이터셋 LMR을 3.1절에서 소개합니다. 그 후, 3.2절에서 여러 참조를 사용하는 새로운 기준 RefSR 방법인 MRefSR을 자세히 설명합니다.

## 3.1. Construction of LMR

MegaDepth \[17\] 데이터셋은 원래 단일 뷰 깊이 예측을 위해 제안되었습니다. 이들은 COLMAP, 최첨단 SfM 시스템 \[24\] (카메라 포즈와 희소 포인트 클라우드 복원을 위해) 및 MVS 시스템 \[25\] (밀집 깊이 맵 생성을 위해)을 사용하여 겹치는 뷰포인트의 인터넷 이미지를 통해 밀집 깊이를 얻었습니다. 생성된 COLMAP의 밀집 깊이 맵은 단일 뷰 깊이 예측 모델 훈련을 위한 감독 목표로 사용됩니다. MegaDepth는 전 세계 랜드마크의 1,070,468개의 인터넷 사진을 포함하며, 이들 사진으로부터 196개의 3D 랜드마크 모델을 재구성합니다. 동일한 랜드마크의 각 사진은 뷰포인트, 장면 범위 및 초점이 맞춰진 건물에서 크게 다릅니다. 3D 재구성을 위해 겹치는 뷰포인트의 인터넷 이미지를 찾는 장면은 대상 이미지를 참조 기반 초해상도 수행을 위해 참조 이미지를 찾는 것과 매우 유사합니다. 이에 영감을 받아, 기성품 MegaDepth 데이터셋의 이미지 그룹은 RefSR 데이터셋을 만드는 데 매우 적합합니다. 따라서, 우리는 LMR이라 명명된 새로운 대규모 다중 참조 RefSR 데이터셋을 제안합니다.

LMR 훈련 이미지 패치 그룹을 구성하기 위해, 우리는 원래의 MegaDepth 데이터셋에서 유사한 이미지 쌍을 얻기 위해 다음의 전처리 단계를 수행합니다.

- 첫째, 중복 이미지를 필터링하기 위해 대상 이미지와 후보 참조 이미지의 PSNR이 30dB 이하이어야 합니다.
- 둘째, 후보 참조 이미지와 대상 이미지는 유사한 내용을 가져야 하며, 이는 희소 3D 포인트 클라우드에서 일치하는 키포인트의 겹침 비율 $R_{olp}$을 조절하여 필터링을 수행합니다.
- 셋째, 참조 이미지와 대상 이미지의 동일한 객체의 크기 비율 $R_s$이 너무 작으면 안 되며, 그렇지 않으면 참조 이미지는 충분한 상세한 텍스처 정보를 제공할 수 없습니다.

우리는 D2-Net \[6\]의 기존 코드를 따라 $R_s$와 $R_{olp}$를 계산합니다. D2-Net은 이미지 매칭과 3D 재구성을 위한 방법입니다.

또한, 우리는 이러한 이미지 쌍에 대해 세 가지 유사도 수준을 정의합니다: 높은 유사도 (H), 중간 유사도 (M) 및 낮은 유사도 (L). 이미지 쌍이 $R_{olp}$가 30% 이상이고 크기 비율 $R_s$가 0.9 이상이면 H로 분류됩니다. $R_{olp}$가 10% 이상이고 $R_s$가 0.66 이상이면 M으로 분류됩니다. 그렇지 않으면 L로 분류됩니다.

위의 작업을 통해, 우리는 각 타겟 이미지와 여러 참조 이미지를 포함하는 많은 이미지 그룹을 얻을 수 있습니다. 그러나 GPU 메모리 제한으로 인해 전체 큰 이미지를 네트워크 훈련에 사용하는 것은 종종 불가능합니다. SISR의 경우, 훈련을 위해 이미지에서 패치를 무작위로 잘라내는 것이 일반적입니다. RefSR의 경우, 참조 이미지와 대상 이미지의 유사한 내용이 있는 대응하는 패치를 자르는 것이 더 좋습니다. 예를 들어, CUFED5는 11,871 쌍의 160×160 패치를 훈련 세트로 잘라냈습니다. 다중 참조 데이터셋 LMR의 경우, 우리는 먼저 대상 이미지에서 패치를 무작위로 잘라냅니다. 그런 다음, 우리는 잘라낸 패치의 중심점을 3D 희소 포인트 클라우드에 매핑하고, 매핑된 지점 근처의 키포인트 5개를 선택합니다. 이들은 서로 다른 유사도를 가진 5개의 참조 이미지에서 가져온 것입니다 (하나의 H, 두 개의 M, 두 개의 L). 다음으로, 우리는 선택된 키포인트를 중심으로 하여 해당 패치를 자릅니다. 이와 같은 방식으로, 우리는 훈련 세트로 300×300 크기의 112,142 그룹의 패치를 수집합니다. 이는 CUFED5보다 열 배 더 크고 이미지 크기도 훨씬 큽니다. 더 중요한 것은, 각 그룹이 서로 다른 유사도를 가진 5개의 참조 이미지 패치를 포함하고 있다는 점입니다. 몇 가지 대표적인 샘플은 그림 2에 제시되어 있습니다. 4절에서 설명된 바와 같이, LMR 데이터셋으로 훈련된 모델은 다른 RefSR 데이터셋에서도 좋은 일반화 성능을 보여주며, LMR의 효과를 입증합니다.

LMR 훈련 세트 외에도, 우리는 다중 참조 RefSR 테스트를 위한 테스트 세트를 준비합니다. 훈련 세트에 나타난 대상 또는 참조 패치를 포함한 이미지를 제거합니다. 남은 이미지 쌍으로부터, 우리는 142개의 그룹으로 구성된 테스트 세트를 구성하며, 각 그룹은 하나의 대상 이미지와 2~6개의 참조 이미지를 포함하고 있으며, 이미지 한 변의 길이는 800~1600 사이입니다.

## 3.2. Multi-Reference RefSR network

LMR 데이터셋을 바탕으로, 우리는 여러 참조 이미지를 잘 활용하기 위해 다중 참조 RefSR 네트워크인 MRefSR을 제안합니다. 우리의 MRefSR은 현재 성능이 가장 좋고 시작하기 쉬운 오픈 소스 방법인 $C^2$-Matching \[12\]을 기반으로 합니다. 다른 RefSR 프레임워크인 TTSR \[35\]도 적용 가능합니다. 우리는 단일 참조 특징 전이 대신 다중 참조 특징을 활용하고자 합니다. $C^2$-Matching이 했듯이, Content Extractor (CE)를 사용하여 LR 이미지에서 특징 $F_{LR}$를 추출합니다. 다중 스케일 (1×, 2×, 4×) 참조 특징 $F_{Ref_i}^s$는 VGG 추출기를 사용하여 추출됩니다. 여기서 $s = 1, 2, 4$이고 $i \in \{1, 2, ..., N\}$이며, $N$은 참조 이미지의 수입니다. 간결성을 위해, 이하에서는 $s$를 생략하고, $F_{Ref_i}$를 $F_{Ref_i}^s$ 대신 사용합니다. 사전 훈련된 Contrastive Correspondence Network (CCN)은 LR 입력의 상대적 대상 오프셋 $O_i$와 해당하는 여러 참조 이미지를 얻기 위해 사용됩니다. 이후, 그림 3에서 보여지듯이, 우리는 다중 참조 특징 융합을 위한 Multi-Reference Attention Module (MAM)과 융합된 특징 선택을 위한 Spatial Aware Filtering Module (SAFM)을 개발합니다.

Dynamic Aggregation Module in $C^2$-Matching은 대응하는 사전 오프셋 $O_i$에 의해 참조 특징 $F_{Ref_i}$로부터 정렬된 특징 $F_{ai}$를 얻기 위해 사용됩니다. 그 후, 우리는 MAM을 도입하여 다른 참조 이미지로부터 정렬된 특징을 융합합니다. 자세히 말하자면, 각 특징 스케일에서, 우리는 먼저 정렬된 $N$개의 참조 이미지의 특징에 대한 대응하는 주의(attention) 맵을 생성합니다:

 $$att_i(x, y) = softmax(\langle Q(x, y), K_i(x, y) \rangle) = \frac{\exp(\langle Q(x, y), K_i(x, y) \rangle)}{\sum_{j=1}^N \exp(\langle Q(x, y), K_j(x, y) \rangle)}.$$

우리는 내적을 사용하여 지점 $(x, y)$에서 특징 $Q(x, y)$와 $K_i(x, y)$ 사이의 유사성을 측정합니다. 여기서 쿼리 $Q$는 LR 입력 특징 $F_{LR}$로부터 얻어지고, 키 $K_i$와 값 $V_i$는 $i$번째 참조 이미지에서 정렬된 특징 $F_{ai}$로부터 얻어집니다:

$$ Q = conv_q(F_{LR}), $$
 $$K_i = conv_k(F_{ai}), $$
 $$V_i = conv_v(F_{ai}), $$

여기서 $conv_q$, $conv_k$, $conv_v$는 커널 크기 3×3 및 스트라이드 1의 합성곱(convolutions)입니다. 그런 다음, 우리는 모든 참조 이미지로부터 융합된 참조 특징 $F_{fref}$를 얻습니다:

$$F_{fref}(x, y) = \sum_{i=1}^N (att_i(x, y) \cdot V_i(x, y)).$$

제안된 MAM은 훈련 및 테스트 단계에서 임의의 수의 참조 이미지를 처리할 수 있게 하여, MRefSR을 실제 응용에 더 유연하게 만듭니다.

모든 LR 특징 픽셀이 참조 특징과 잘 일치하지 않을 수 있기 때문에, 우리는 SAFM을 사용하여 융합된 참조 특징 $F_{fref}$를 선택합니다. 그림 3에서 보이듯이, 우리는 $F_{LR}$와 $F_{fref}$의 연결된 특징으로부터 두 개의 마스크 $M_{mul}$과 $M_{add}$를 얻고, 시그모이드 함수를 사용하여 $M_{mul}$의 범위를 제한합니다:

$$M_{mul} = sigmoid(f_1(F_{LR} \parallel F_{fref})) \cdot 2, $$
$$M_{add} = f_2(F_{LR} \parallel F_{fref}),$$

여기서 $f_1$과 $f_2$는 합성곱과 leaky ReLU 층으로 구성된 비선형 매핑 함수들입니다. 마지막으로, $M_{mul}$과 $M_{add}$는 최종 선택된 참조 특징 $F_{sref}$를 위해 사용됩니다:

$$F_{sref} = F_{fref} \circ M_{mul} + M_{add},$$

여기서 $\circ$는 원소별 곱셈을 나타냅니다.

마지막으로, 복원 모듈 $G$는 LR 특징 $F_{LR}$과 선택된 참조 특징 $F_{sref}$를 가져와 타겟 이미지를 복원합니다:

$$X_{SR} = G(F_{LR}, F_{sref}).$$

다음은 주어진 텍스트의 번역입니다.

## 3.3. Implementation Details

우리는 MRefSR을 스케일 팩터 4×에서 훈련하고 평가합니다. 자세히 말하자면, 우리는 네트워크를 Adam optimizer \[15\]를 사용하여 255K 반복으로 훈련합니다. 매개변수는 $\beta_1 = 0.9$, $\beta_2 = 0.999$, 그리고 학습률은 1e-4로 설정합니다. 각 미니 배치는 48 그룹의 이미지 패치를 포함하며, 각각의 그룹은 크기 40×40의 LR 입력 패치와 크기 160×160의 다섯 개의 참조 HR 패치로 구성됩니다. 우리는 모델을 훈련시키기 위해 세 가지 일반적으로 사용되는 손실 함수, 즉 재구성 손실 $L_{rec}$, 지각 손실 $L_{per}$, 그리고 적대적 손실 $L_{adv}$를 사용합니다. 네트워크 훈련 손실 세부 사항은 보충 자료를 참조하십시오. $L_{rec}$, $L_{per}$, $L_{adv}$의 가중치 계수는 각각 1, 1e-4, 1e-6으로 설정됩니다. 네트워크는 먼저 $L_{rec}$로만 훈련된 후, 모든 손실로 미세 조정됩니다. 훈련 중에는, 우리는 훈련 데이터를 무작위로 수평 뒤집기, 수직 뒤집기, 그리고 90도 회전으로 증강합니다. 표준 프로토콜을 따르며, 우리는 HR 이미지를 4×의 스케일 팩터로 양방향적 다운샘플링하여 모든 LR 이미지를 생성합니다. 모든 실험은 4개의 NVIDIA V100 GPU에서 병렬로 실행됩니다. 정량적 비교를 위해, 우리는 GAN 손실과 지각 손실 없이 MRefSR을 다른 방법들처럼 훈련시킵니다. LMR의 고해상도 훈련 이미지의 혜택을 받아, 우리는 모델의 LPF (Large Patch Fine-tuning) 버전을 얻었으며, 이는 큰 패치 훈련 이미지를 사용하여 미세 조정된 것입니다.